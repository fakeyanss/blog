<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hadoop入门]]></title>
    <url>%2Farchives%2F67914616.html</url>
    <content type="text"><![CDATA[Understanding Hadoop Clusters and the NetworkAuthor: Brad Hedlund Link: original text Translator: Yanss This article is Part 1 in series that will take a closer look at the architecture and methods of a Hadoop cluster, and how it relates to the network and server infrastructure. The content presented here is largely based on academic work and conversations I’ve had with customers running real production clusters. If you run production Hadoop clusters in your data center, I’m hoping you’ll provide your valuable insight in the comments below. Subsequent articles to this will cover the server and network architecture options in closer detail. Before we do that though, lets start by learning some of the basics about how a Hadoop cluster works. OK, let’s get started! The three major categories of machine roles in a Hadoop deployment are Client machines, Masters nodes, and Slave nodes. The Master nodes oversee the two key functional pieces that make up Hadoop: storing lots of data (HDFS), and running parallel computations on all that data (Map Reduce). The Name Node oversees and coordinates the data storage function (HDFS), while the Job Tracker oversees and coordinates the parallel processing of data using Map Reduce. Slave Nodes make up the vast majority of machines and do all the dirty work of storing the data and running the computations. Each slave runs both a Data Node and Task Tracker daemon that communicate with and receive instructions from their master nodes. The Task Tracker daemon is a slave to the Job Tracker, the Data Node daemon a slave to the Name Node. Client machines have Hadoop installed with all the cluster settings, but are neither a Master or a Slave. Instead, the role of the Client machine is to load data into the cluster, submit Map Reduce jobs describing how that data should be processed, and then retrieve or view the results of the job when its finished. In smaller clusters (~40 nodes) you may have a single physical server playing multiple roles, such as both Job Tracker and Name Node. With medium to large clusters you will often have each role operating on a single server machine. In real production clusters there is no server virtualization, no hypervisor layer. That would only amount to unnecessary overhead impeding performance. Hadoop runs best on Linux machines, working directly with the underlying hardware. That said, Hadoop does work in a virtual machine. That’s a great way to learn and get Hadoop up and running fast and cheap. I have a 6-node cluster up and running in VMware Workstation on my Windows 7 laptop. This is the typical architecture of a Hadoop cluster. You will have rack servers (not blades) populated in racks connected to a top of rack switch usually with 1 or 2 GE boned links. 10GE nodes are uncommon but gaining interest as machines continue to get more dense with CPU cores and disk drives. The rack switch has uplinks connected to another tier of switches connecting all the other racks with uniform bandwidth, forming the cluster. The majority of the servers will be Slave nodes with lots of local disk storage and moderate amounts of CPU and DRAM. Some of the machines will be Master nodes that might have a slightly different configuration favoring more DRAM and CPU, less local storage. In this post, we are not going to discuss various detailed network design options. Let’s save that for another discussion (stay tuned). First, lets understand how this application works… Why did Hadoop come to exist? What problem does it solve? Simply put, businesses and governments have a tremendous amount of data that needs to be analyzed and processed very quickly. If I can chop that huge chunk of data into small chunks and spread it out over many machines, and have all those machines processes their portion of the data in parallel – I can get answers extremely fast – and that, in a nutshell, is what Hadoop does. In our simple example, we’ll have a huge data file containing emails sent to the customer service department. I want a quick snapshot to see how many times the word “Refund” was typed by my customers. This might help me to anticipate the demand on our returns and exchanges department, and staff it appropriately. It’s a simple word count exercise. The Client will load the data into the cluster (File.txt), submit a job describing how to analyze that data (word count), the cluster will store the results in a new file (Results.txt), and the Client will read the results file. Your Hadoop cluster is useless until it has data, so we’ll begin by loading our huge File.txt into the cluster for processing. The goal here is fast parallel processing of lots of data. To accomplish that I need as many machines as possible working on this data all at once. To that end, the Client is going to break the data file into smaller “Blocks”, and place those blocks on different machines throughout the cluster. The more blocks I have, the more machines that will be able to work on this data in parallel. At the same time, these machines may be prone to failure, so I want to insure that every block of data is on multiple machines at once to avoid data loss. So each block will be replicated in the cluster as its loaded. The standard setting for Hadoop is to have (3) copies of each block in the cluster. This can be configured with the dfs.replication parameter in the file hdfs-site.xml. The Client breaks File.txt into (3) Blocks. For each block, the Client consults the Name Node (usually TCP 9000) and receives a list of (3) Data Nodes that should have a copy of this block. The Client then writes the block directly to the Data Node (usually TCP 50010). The receiving Data Node replicates the block to other Data Nodes, and the cycle repeats for the remaining blocks. The Name Node is not in the data path. The Name Node only provides the map of where data is and where data should go in the cluster (file system metadata). Hadoop has the concept of “Rack Awareness”. As the Hadoop administrator you can manually define the rack number of each slave Data Node in your cluster. Why would you go through the trouble of doing this? There are two key reasons for this: Data loss prevention, and network performance. Remember that each block of data will be replicated to multiple machines to prevent the failure of one machine from losing all copies of data. Wouldn’t it be unfortunate if all copies of data happened to be located on machines in the same rack, and that rack experiences a failure? Such as a switch failure or power failure. That would be a mess. So to avoid this, somebody needs to know where Data Nodes are located in the network topology and use that information to make an intelligent decision about where data replicas should exist in the cluster. That “somebody” is the Name Node. There is also an assumption that two machines in the same rack have more bandwidth and lower latency between each other than two machines in two different racks. This is true most of the time. The rack switch uplink bandwidth is usually (but not always) less than its downlink bandwidth. Furthermore, in-rack latency is usually lower than cross-rack latency (but not always). If at least one of those two basic assumptions are true, wouldn’t it be cool if Hadoop can use the same Rack Awareness that protects data to also optimally place work streams in the cluster, improving network performance? Well, it does! Cool, right? What is NOT cool about Rack Awareness at this point is the manual work required to define it the first time, continually update it, and keep the information accurate. If the rack switch could auto-magically provide the Name Node with the list of Data Nodes it has, that would be cool. Or vice versa, if the Data Nodes could auto-magically tell the Name Node what switch they’re connected to, that would be cool too. Even more interesting would be a OpenFlow network, where the Name Node could query the OpenFlow controller about a Node’s location in the topology. The Client is ready to load File.txt into the cluster and breaks it up into blocks, starting with Block A. The Client consults the Name Node that it wants to write File.txt, gets permission from the Name Node, and receives a list of (3) Data Nodes for each block, a unique list for each block. The Name Node used its Rack Awareness data to influence the decision of which Data Nodes to provide in these lists. The key rule is that for every block of data, two copies will exist in one rack, another copy in a different rack. So the list provided to the Client will follow this rule. Before the Client writes “Block A” of File.txt to the cluster it wants to know that all Data Nodes which are expected to have a copy of this block are ready to receive it. It picks the first Data Node in the list for Block A (Data Node 1), opens a TCP 50010 connection and says, “Hey, get ready to receive a block, and here’s a list of (2) Data Nodes, Data Node 5 and Data Node 6. Go make sure they’re ready to receive this block too.” Data Node 1 then opens a TCP connection to Data Node 5 and says, “Hey, get ready to receive a block, and go make sure Data Node 6 is ready is receive this block too.” Data Node 5 will then ask Data Node 6, “Hey, are you ready to receive a block?” The acknowledgments of readiness come back on the same TCP pipeline, until the initial Data Node 1 sends a “Ready” message back to the Client. At this point the Client is ready to begin writing block data into the cluster. As data for each block is written into the cluster a replication pipeline is created between the (3) Data Nodes (or however many you have configured in dfs.replication). This means that as a Data Node is receiving block data it will at the same time push a copy of that data to the next Node in the pipeline. Here too is a primary example of leveraging the Rack Awareness data in the Name Node to improve cluster performance. Notice that the second and third Data Nodes in the pipeline are in the same rack, and therefore the final leg of the pipeline does not need to traverse between racks and instead benefits from in-rack bandwidth and low latency. The next block will not be begin until this block is successfully written to all three nodes. When all three Nodes have successfully received the block they will send a “Block Received” report to the Name Node. They will also send “Success” messages back up the pipeline and close down the TCP sessions. The Client receives a success message and tells the Name Node the block was successfully written. The Name Node updates it metadata info with the Node locations of Block A in File.txt. The Client is ready to start the pipeline process again for the next block of data. As the subsequent blocks of File.txt are written, the initial node in the pipeline will vary for each block, spreading around the hot spots of in-rack and cross-rack traffic for replication. Hadoop uses a lot of network bandwidth and storage. We are typically dealing with very big files, Terabytes in size. And each file will be replicated onto the network and disk (3) times. If you have a 1TB file it will consume 3TB of network traffic to successfully load the file, and 3TB disk space to hold the file. After the replication pipeline of each block is complete the file is successfully written to the cluster. As intended the file is spread in blocks across the cluster of machines, each machine having a relatively small part of the data. The more blocks that make up a file, the more machines the data can potentially spread. The more CPU cores and disk drives that have a piece of my data mean more parallel processing power and faster results. This is the motivation behind building large, wide clusters. To process more data, faster. When the machine count goes up and the cluster goes wide, our network needs to scale appropriately. Another approach to scaling the cluster is to go deep. This is where you scale up the machines with more disk drives and more CPU cores. Instead of increasing the number of machines you begin to look at increasing the density of each machine. In scaling deep, you put yourself on a trajectory where more network I/O requirements may be demanded of fewer machines. In this model, how your Hadoop cluster makes the transition to 10GE nodes becomes an important consideration. The Name Node holds all the file system metadata for the cluster and oversees the health of Data Nodes and coordinates access to data. The Name Node is the central controller of HDFS. It does not hold any cluster data itself. The Name Node only knows what blocks make up a file and where those blocks are located in the cluster. The Name Node points Clients to the Data Nodes they need to talk to and keeps track of the cluster’s storage capacity, the health of each Data Node, and making sure each block of data is meeting the minimum defined replica policy. Data Nodes send heartbeats to the Name Node every 3 seconds via a TCP handshake, using the same port number defined for the Name Node daemon, usually TCP 9000. Every tenth heartbeat is a Block Report, where the Data Node tells the Name Node about all the blocks it has. The block reports allow the Name Node build its metadata and insure (3) copies of the block exist on different nodes, in different racks. The Name Node is a critical component of the Hadoop Distributed File System (HDFS). Without it, Clients would not be able to write or read files from HDFS, and it would be impossible to schedule and execute Map Reduce jobs. Because of this, it’s a good idea to equip the Name Node with a highly redundant enterprise class server configuration; dual power supplies, hot swappable fans, redundant NIC connections, etc. If the Name Node stops receiving heartbeats from a Data Node it presumes it to be dead and any data it had to be gone as well. Based on the block reports it had been receiving from the dead node, the Name Node knows which copies of blocks died along with the node and can make the decision to re-replicate those blocks to other Data Nodes. It will also consult the Rack Awareness data in order to maintain the two copies in one rack, one copy in another rack replica rule when deciding which Data Node should receive a new copy of the blocks. Consider the scenario where an entire rack of servers falls off the network, perhaps because of a rack switch failure, or power failure. The Name Node would begin instructing the remaining nodes in the cluster to re-replicate all of the data blocks lost in that rack. If each server in that rack had a modest 12TB of data, this could be hundreds of terabytes of data that needs to begin traversing the network. Hadoop has server role called the Secondary Name Node. A common misconception is that this role provides a high availability backup for the Name Node. This is not the case. The Secondary Name Node occasionally connects to the Name Node (by default, ever hour) and grabs a copy of the Name Node’s in-memory metadata and files used to store metadata (both of which may be out of sync). The Secondary Name Node combines this information in a fresh set of files and delivers them back to the Name Node, while keeping a copy for itself. Should the Name Node die, the files retained by the Secondary Name Node can be used to recover the Name Node. In a busy cluster, the administrator may configure the Secondary Name Node to provide this housekeeping service much more frequently than the default setting of one hour. Maybe every minute. When a Client wants to retrieve a file from HDFS, perhaps the output of a job, it again consults the Name Node and asks for the block locations of the file. The Name Node returns a list of each Data Node holding a block, for each block. The Client picks a Data Node from each block list and reads one block at a time with TCP on port 50010, the default port number for the Data Node daemon. It does not progress to the next block until the previous block completes. There are some cases in which a Data Node daemon itself will need to read a block of data from HDFS. One such case is where the Data Node has been asked to process data that it does not have locally, and therefore it must retrieve the data from another Data Node over the network before it can begin processing. This is another key example of the Name Node’s Rack Awareness knowledge providing optimal network behavior. When the Data Node asks the Name Node for location of block data, the Name Node will check if another Data Node in the same rack has the data. If so, the Name Node provides the in-rack location from which to retrieve the data. The flow does not need to traverse two more switches and congested links find the data in another rack. With the data retrieved quicker in-rack, the data processing can begin sooner, and the job completes that much faster. Now that File.txt is spread in small blocks across my cluster of machines I have the opportunity to provide extremely fast and efficient parallel processing of that data. The parallel processing framework included with Hadoop is called Map Reduce, named after two important steps in the model; Map, and Reduce. The first step is the Map process. This is where we simultaneously ask our machines to run a computation on their local block of data. In this case we are asking our machines to count the number of occurrences of the word “Refund” in the data blocks of File.txt. To start this process the Client machine submits the Map Reduce job to the Job Tracker, asking “How many times does Refund occur in File.txt” (paraphrasing Java code). The Job Tracker consults the Name Node to learn which Data Nodes have blocks of File.txt. The Job Tracker then provides the Task Tracker running on those nodes with the Java code required to execute the Map computation on their local data. The Task Tracker starts a Map task and monitors the tasks progress. The Task Tracker provides heartbeats and task status back to the Job Tracker. As each Map task completes, each node stores the result of its local computation in temporary local storage. This is called the “intermediate data”. The next step will be to send this intermediate data over the network to a Node running a Reduce task for final computation. While the Job Tracker will always try to pick nodes with local data for a Map task, it may not always be able to do so. One reason for this might be that all of the nodes with local data already have too many other tasks running and cannot accept anymore. In this case, the Job Tracker will consult the Name Node whose Rack Awareness knowledge can suggest other nodes in the same rack. The Job Tracker will assign the task to a node in the same rack, and when that node goes to find the data it needs the Name Node will instruct it to grab the data from another node in its rack, leveraging the presumed single hop and high bandwidth of in-rack switching. The second phase of the Map Reduce framework is called, you guess it, Reduce. The Map task on the machines have completed and generated their intermediate data. Now we need to gather all of this intermediate data to combine and distill it for further processing such that we have one final result. The Job Tracker starts a Reduce task on any one of the nodes in the cluster and instructs the Reduce task to go grab the intermediate data from all of the completed Map tasks. The Map tasks may respond to the Reducer almost simultaneously, resulting in a situation where you have a number of nodes sending TCP data to a single node, all at once. This traffic condition is often referred to as TCP Incast or “fan-in”. For networks handling lots of Incast conditions, it’s important the network switches have well-engineered internal traffic management capabilities, and adequate buffers (not too big, not too small). Throwing gobs of buffers at a switch may end up causing unwanted collateral damage to other traffic. But that’s a topic for another day. The Reducer task has now collected all of the intermediate data from the Map tasks and can begin the final computation phase. In this case, we are simply adding up the sum total occurrences of the word “Refund” and writing the result to a file called Results.txt The output from the job is a file called Results.txt that is written to HDFS following all of the processes we have covered already; splitting the file up into blocks, pipeline replication of those blocks, etc. When complete, the Client machine can read the Results.txt file from HDFS, and the job is considered complete. Our simple word count job did not result in a lot of intermediate data to transfer over the network. Other jobs however may produce a lot of intermediate data – such as sorting a terabyte of data. Where the output of the Map Reduce job is a new set of data equal to the size of data you started with. How much traffic you see on the network in the Map Reduce process is entirely dependent on the type job you are running at that given time. If you’re a studious network administrator, you would learn more about Map Reduce and the types of jobs your cluster will be running, and how the type of job affects the traffic flows on your network. If you’re a Hadoop networking rock star, you might even be able to suggest ways to better code the Map Reduce jobs so as to optimize the performance of the network, resulting in faster job completion times. Hadoop may start to be a real success in your organization, providing a lot of previously untapped business value from all that data sitting around. When business folks find out about this you can bet that you’ll quickly have more money to buy more racks of servers and network for your Hadoop cluster. When you add new racks full of servers and network to an existing Hadoop cluster you can end up in a situation where your cluster is unbalanced. In this case, Racks 1 &amp; 2 were my existing racks containing File.txt and running my Map Reduce jobs on that data. When I added two new racks to the cluster, my File.txt data doesn’t auto-magically start spreading over to the new racks. All the data stays where it is. The new servers are sitting idle with no data, until I start loading new data into the cluster. Furthermore, if the servers in Racks 1 &amp; 2 are really busy, the Job Tracker may have no other choice but to assign Map tasks on File.txt to the new servers which have no local data. The new servers need to go grab the data over the network. As as result you may see more network traffic and slower job completion times. To fix the unbalanced cluster situation, Hadoop includes a nifty utility called, you guessed it, balancer. Balancer looks at the difference in available storage between nodes and attempts to provide balance to a certain threshold. New nodes with lots of free disk space will be detected and balancer can begin copying block data off nodes with less available space to the new nodes. Balancer isn’t running until someone types the command at a terminal, and it stops when the terminal is canceled or closed. The amount of network traffic balancer can use is very low, with a default setting of 1MB/s. This setting can be changed with the dfs.balance.bandwidthPerSec parameter in the file hdfs-site.xml The Balancer is good housekeeping for your cluster. It should definitely be used any time new machines are added, and perhaps even run once a week for good measure. Given the balancers low default bandwidth setting it can take a long time to finish its work, perhaps days or weeks. Wouldn’t it be cool if cluster balancing was a core part of Hadoop, and not just a utility? I think so. This material is based on studies, training from Cloudera, and observations from my own virtual Hadoop lab of six nodes. Everything discussed here is based on the latest stable release of Cloudera’s CDH3 distribution of Hadoop. There are new and interesting technologies coming to Hadoop such as Hadoop on Demand (HOD) and HDFS Federations, not discussed here, but worth investigating on your own if so inclined. Download: Slides - PDF Slides and Text - PDF Cheers, Brad 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Coding动态pages部署WordPress]]></title>
    <url>%2Farchives%2F638b585f.html</url>
    <content type="text"><![CDATA[什么都别说了，先上图 部署好的wordpress效果（我主要是做一个相册集） 总体效果还不错吧，毕竟动态博客还是可操作性强多了。 好了，进入正题。 做这个其实就是前几天在coding的pages服务菜单中发现还有静态和动态两个选项卡，当时就懵逼了，pages服务还能动态？ 然后看了说明，动态是可以，但是限制还是蛮多的。 动态 Pages 是一个动态网页托管和演示服务，支持使用 PHP 语言和 MySQL 数据库，可用于部署开源博客、CMS 等动态应用。 只能使用php语言，然后数据库其实是coding自己的服务器提供的，然后整个服务器后台也是coding提供，所以自己是不可能做什么修改的。当然做一个wordpress博客还是绰绰有余，下面就是我的wordpress仓库文件。 搭建过程也非常简单，就是coding新建一个仓库，然后去wordpress官网上下载最新的wordpress压缩包，解压之后push到coding仓库中。 然后在Pages服务中开启动态Pages，选择部署来源为master分支，稍等一下就自动部署完成了。 打开动态pages运行的url，然后就是5分钟流程了。 所有的连接信息(共5个)都在这里，只有前4个用得上 存在的问题： 使用过程中在wordpress管理后台中下载好了主题和插件，但是在coding仓库中却没有对应的文件增加。所以在偶尔出现数据库连接错误或其他问题需要重新部署是，原本设置好的插件和主题就没有了 由于动态pages使用的是coding自己的服务器，所以个人没法修改服务器的一些设置，比如上传文件的大小限制，图片的分辨率等，所以上传的大图需要自己压缩一下再传。 最后建议： 把连接信息保存到一个文件wp-config.php，放到根目录下。 所有的主题和插件去源网页下载文件，保存到wp-content下的对应文件夹下，然后提交到coding 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>WordPress</tag>
        <tag>Coding-pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitBook一键发布到gh-pages]]></title>
    <url>%2Farchives%2Fb2973de0.html</url>
    <content type="text"><![CDATA[背景这段时间在读一本英文书，读的很慢，可以说是逐词翻译了。 然而读的时候总是会忘了前面的生词是什么意思，也没有纸质打印版，所以想到边读边做笔记，主要就是生词注释一下。 于是想到了GitBook。 首先我在GitBook上创建一本书，书名是Hadoop-The Definitive Guide 4th Edition。 然后打开就可以直接编辑。 但是gitbook的编辑器很难用啊，好像原来是直接写markdown的，现在改了编辑模式？特别是插入连接的时候，没法像[]()这么方便啊。 而且重要的是gitbook服务器加载速度不稳定，慢的时候都打不开了，所以想着直接把gitbook的Markdown文件内容编译成静态页面，发布到github仓库中，利用gh-pages直接访问，速度快多了。 连接github仓库现在说说怎么部署到gh-pages。 首先在github创建一个仓库，Hadoop-The-Definitive-Guide-4th，并初始化。 然后到gitbook的书籍Hadoop-The Definitive Guide 4th Edition的设置里找到Github，添加对应Hadoop-The-Definitive-Guide-4th仓库并同步内容 之后可以在github仓库中看到一些文件 这些都是gitbook书的markdown文件。这一步完成后，就可以在gitbook或github任意一端编译文档，提交后都会在两端生成的相应书籍。这相当于书籍在两端都有备份了。 如果不用gh-pages生成页面的话，上述的操作就已经够了。 提交gh-pages分支接下来介绍如何提交静态页面到gh-pages。 由于要生成静态页面的文件，需要在本地安装gitbook的npm包(推荐使用cnpm安装)。1npm install gitbook-cli -g 然后把github仓库clone到本地1git clone git@github.com:fakeYanss/Hadoop-The-Definitive-Guide-4th.git 进入到Hadoop-The-Definitive-Guide-4th文件夹，生成静态页面文件，输出目录在_book中。如果目录文件SUMMARY.md有变化，需要先gitbook init。1gitbook build 然后在本地创建一个gh-pages分支1git checkout --orphan gh-pages 然后清空一下分支下的文件（如果有的话）1rm -rf * 然后将master分支下的_book静态页面文件内容全部复制到gh-pages分支下1git checkout master -- _book 将_book中的子文件全部移到外层，并删除_book12mv _book/* ./rm -rf _book 这时候gh-pages分支下就是全部的静态页面文件了，接下来就是提交到远程gh-pages分支123git add .git commit -m &apos;publish gh-pages&apos;git push origin gh-pages 提交完成后到github仓库的设置中看一下，gh-pages服务是否自动开启，如果没有的话在Source中选择gh-pages branch，保存刷新，等待几分钟就好了。 全部操作已经完成，接下来每次在本地更新书籍内容后，先生成静态页面，然后提交master分支，再提交gh-pages分支就可以了。 之后每次查看线上gitbook书籍，可以直接输入url https://name.github.io/书籍仓库名查看。 最后，为了每次的提交操作不用手打一遍，我写了一个bash脚本publish.sh，点击下载，自行更改第一行的文件夹地址即可。windows系统安装过git环境的可以直接双击运行，要查看日志的话可以在git bash中输入./publish.sh运行。 注意：使用时不能将脚本放在仓库里，不然在切换分支时会出错，最好与仓库文件夹同级。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>GitBook</tag>
        <tag>gh-pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms_Java题解]]></title>
    <url>%2Farchives%2Fbc89f3b9.html</url>
    <content type="text"><![CDATA[以下是做过的LeetCode算法题，Java语言书写。全部题解是一个Java Project, 环境jdk 1.8.0，可直接运行。 1-1001. Two Sum 28. Implement_strStr 101-200125. Valid_Palindrome 151. Reverse Words in a String 167. Two_Sum_II 170. Two_Sum_III 201-300301-400338. Counting_Bits 344. Reverse_String 401-500412. Fizz_Buzz 419. Battleships_in_a_Board 461. Hamming_Distance 476. Number_Complement 500. Keyboard_Row 501-600535. Encode_and_Decode_TinyURL 537. Complex_Number_Multiplication 557. Reverse_Words_in_a_String_III 561. Array_Partition_I 566. Reshape_the_Matrix 601-700617. Merge_two_Binary_Trees 654. Maximum_Binary_Tree 657. Judge_Route_Circle 669. Trim_a_Binary_Search_Tree 682. Baseball_Game 701-800728. Self_Dividing_Numbers 760. Find_Anagram_Mappings 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThoughtWorks笔试题]]></title>
    <url>%2Farchives%2Ffd067be5.html</url>
    <content type="text"><![CDATA[2018_SPRING_DEV题目下载 解决思路 首先需要一个逐行读取文件内容的方法， 构造文件输入流，再构造Scanner类输入即可。然后将读取的逐行信息切分为一个数组，保存到ArrayList1中；再以ArrayList2嵌套ArrayList1即可 在main方法中获取键盘输入信息作为消息序号id，然后遍历从第0条到第id条消息，得出第id条消息的输出 步骤 新建一个input.txt文件记录无人机活动信号 12345plane1 1 1 1plane1 1 1 1 1 2 3plane1 2 3 4 1 1 1plane1 3 4 5plane1 1 1 1 1 2 3 新建一个PositionOfPlane.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import java.io.FileInputStream;import java.io.FileNotFoundException;import java.util.ArrayList;import java.util.List;import java.util.Scanner;import java.util.regex.Pattern;/** * Auther: 桂晨 * Date: 2018年1月23日00:14:33 * */public class PositionOfPlane &#123; public static String PLANEID = ""; //无人机编号 public static String PATH = "input.txt"; //记录无人机活动信号的文本文件路径 public static void main(String args[]) &#123; List&lt;List&lt;Integer&gt;&gt; plane; try &#123; plane = ReadFile(PATH); System.out.println("请输入消息序号(自然数):"); Scanner sc = new Scanner(System.in); boolean flag = true; String _id = ""; int id; //判断输入是不是自然数 while(flag) &#123; _id = sc.next(); Pattern pattern = Pattern.compile("[0-9]*"); if(pattern.matcher(_id).matches())&#123; flag = false; &#125;else&#123; System.out.println("请重新输入"); &#125; &#125; id = Integer.valueOf(_id); //将输入消息序号id分为三种情况，0，超出数据集，和在数据集中(不为0) if (id == 0) &#123; System.out.println(PLANEID + " " + id + " " + plane.get(0).get(0) + " " + plane.get(0).get(1) + " " + plane.get(0).get(2)); &#125; else if (id &gt; plane.size() - 1) &#123; System.out.println("Cannot find " + id); &#125; else &#123; Print(id, plane); &#125; &#125; catch (FileNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; //输入消息序号id和信号数据plane，打印结果 public static void Print(int id, List&lt;List&lt;Integer&gt;&gt; plane) &#123; int num = 1; int x, y, z, offsetx, offsety, offsetz; x = plane.get(0).get(0); y = plane.get(0).get(1); z = plane.get(0).get(2); while (num &lt;= id) &#123; if (plane.get(num).size() != 6 || x != plane.get(num).get(0) || y != plane.get(num).get(1) || z != plane.get(num).get(2)) &#123; System.out.println("Error: " + id); return; &#125; offsetx = plane.get(num).get(3); offsety = plane.get(num).get(4); offsetz = plane.get(num).get(5); x += offsetx; y += offsety; z += offsetz; num++; &#125; System.out.println(PLANEID + " " + id + " " + x + " " + y + " " + z); &#125; //使用Scanner类nextLine()方法，读取文件每一行的数据，并将每个数据切分，保存到List&lt;List&lt;&gt;&gt;的嵌套集合(动态二维数组)中 public static List&lt;List&lt;Integer&gt;&gt; ReadFile(String path) throws FileNotFoundException &#123; FileInputStream fis = new FileInputStream(path); Scanner scanner = new Scanner(fis); String[] str; List&lt;List&lt;Integer&gt;&gt; plane = new ArrayList&lt;List&lt;Integer&gt;&gt;(); List&lt;Integer&gt; col = new ArrayList&lt;Integer&gt;(); while (scanner.hasNextLine()) &#123; str = scanner.nextLine().split(" "); for (int i = 1; i &lt; str.length; i++) &#123; col.add(Integer.parseInt(str[i])); &#125; PLANEID = str[0]; plane.add(new ArrayList&lt;Integer&gt;(col)); col.clear(); &#125; scanner.close(); return plane; &#125;&#125; 运行先编译生成字节码 1javac -encoding utf-8 PositionOfPlane.java 然后运行 1java PositionOfPlane 然后输入ID 12（或其他数字） 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>ThoughtWorks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java不用for循环打印数组]]></title>
    <url>%2Farchives%2F8576f9e7.html</url>
    <content type="text"><![CDATA[ArrayList直接打印123456ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;();list.add(1);list.add(2);list.add(3);list.add(4);System.out.println(list); 输出1[1, 2, 3, 4] Arrays类打印数组java.util.Arrays的toString()方法1System.out.println(Arrays.toString(new int[] &#123;1, 2, 3, 4&#125;)); 输出1[1, 2, 3, 4] Arrays类打印二维数组java.util.Arrays的deeptoString()方法1System.out.println(Arrays.deepToString(new int[][] &#123;&#123;1, 2&#125;, &#123;3, 4&#125;&#125;)); 输出1[[1, 2], [3, 4]] 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识机器学习]]></title>
    <url>%2Farchives%2Fcc2e4451.html</url>
    <content type="text"><![CDATA[Machine learningMachine learning definition: Arthur Samuel (1959) . 在没有明确设置的前提下，使机器具有学习能力的研究领域。 Tom Mitchell (1998) . 一个适当的学习问题定义如下：计算机程序从经验E中学习，解决某一任务T，进行某一性能度量P，通过P测试在T上的表现因经验E而提高。 对于跳棋游戏（Samuel设计的一个小游戏，通过数万次跳棋对战学习，获得比Samuel的跳棋水平还高的能力），经验E就是程序与自己下几万次跳棋，任务T就是玩跳棋，性能度量P就是与新对手玩跳棋时赢的概率。 Machine learning algorithms: 目前学习算法主要的两类是监督学习(supervised learning)和无监督学习(unsupervised learning)。 简单来说，监督学习就是我们教计算机做某件事情；在无监督学习中，我们让计算机自己学习。 Others: 强化学习(Reinforcement learning), 推荐系统(recommender systems) Supervised Learning监督学习：我们给算法一个数据集，其中包含了正确答案，算法的目的就是给出更多的正确答案。 回归(Regression)：预测连续的数值输出。 分类(Classification)：预测一个离散值输出。 示例：房子的价格与房子面积的关系(回归问题)；肿瘤是恶性或良性与肿瘤大小，患者年龄，肿瘤块厚度等的关系(分类问题)。 下面一个问题。problem1将要卖的货物数量看成一个连续的值，属于回归问题；problem2输出的值可能为0或1，分别表示两种不同的结果，属于分类问题。 Unsupervised Learning无监督学习：对于数据集中的每一个样本，都具有相同标签或都没有标签，我们不知道要拿数据做什么，也不知道每个数据点究竟是什么，只能在数据集种找到某种结构(簇)，它们具有类似的性质。聚类(clustering)是无监督学习的一种 。 Cocktail party problem 鸡尾酒会问题 编程环境Octave或Matlab 解决代码$$[W,s,v]=svd((repmat(sum(x.^*x,1),size(x,1),1).^*x)^*x’)$$ \(svd\)是奇异值分解的缩写，在Octave中作为一个内置函数。 下面一个问题，哪些选项要使用无监督学习算法？ 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>supervised learning</tag>
        <tag>unsupervised learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubantu系统安装]]></title>
    <url>%2Farchives%2F67d3a6a5.html</url>
    <content type="text"><![CDATA[Ubantu系统安装教程 Ubantu Desktop最新版本下载，制作U盘启动盘推荐使用Rufus工具 Ubantu系统分区方案 在windows系统上把原硬盘压缩出50G的free空间，在Ubantu安装时分出2G作为swap分区 剩下的格式化为ext4格式，挂载位置为/ 由于现在PC内存都较大了，所以不必创建swap交换分区 设置安装启动引导器的设备 我的电脑有两块ssd，一块小的全部作为C盘，装的win10系统和开机启动软件；一块大的作为D盘，安装常用软件和存放一些资料。我把ubantu安装在了D盘上的一个50G分区，这样就要把ubantu的引导器放在D盘，也就是sdb （sda对应第一块硬盘，sdb对应第二块硬盘），这样的话电脑开机时会自动进入win10，如果我按F11才会进入grub选择ubantu系统，这样正好符合我的需求。 如果把引导器安装在C盘，每次开机都会手动选择系统。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Ubantu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo的next主题个性化配置]]></title>
    <url>%2Farchives%2F14046165.html</url>
    <content type="text"><![CDATA[添加RSS首先在博客根目录安装hexo插件： 1$ npm install --save hexo-generator-feed npm安装失败请用cnpm 然后在博客配置文件_config.yml中修改 12345plugins: hexo-generate-feedfeed: type: atom #feed 类型 (atom/rss2) path: atom.xml #rss 路径 limit: 0 #在 rss 中最多生成的文章数(0显示所有) 然后在主题配置文件_config.yml中修改 1rss: /atom.xml 修改作者头像并旋转打开\themes\next\source\css\_common\components\sidebar\sidebar-author.styl，在里面添加如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960.site-author-image &#123; display: block; margin: 0 auto; padding: $site-author-image-padding; max-width: $site-author-image-width; height: $site-author-image-height; border: $site-author-image-border-width solid $site-author-image-border-color; /* 头像圆形 */ border-radius: 80px; -webkit-border-radius: 80px; -moz-border-radius: 80px; box-shadow: inset 0 -1px 0 #333sf; /* 设置循环动画 [animation: (play)动画名称 (2s)动画播放时长单位秒或微秒 (ase-out)动画播放的速度曲线为以低速结束 (1s)等待1秒然后开始动画 (1)动画播放次数(infinite为循环播放) ]*/ /* 鼠标经过头像旋转360度 */ -webkit-transition: -webkit-transform 1.0s ease-out; -moz-transition: -moz-transform 1.0s ease-out; transition: transform 1.0s ease-out;&#125;img:hover &#123; /* 鼠标经过停止头像旋转 -webkit-animation-play-state:paused; animation-play-state:paused;*/ /* 鼠标经过头像旋转360度 */ -webkit-transform: rotateZ(360deg); -moz-transform: rotateZ(360deg); transform: rotateZ(360deg);&#125;/* Z 轴旋转动画 */@-webkit-keyframes play &#123; 0% &#123; -webkit-transform: rotateZ(0deg); &#125; 100% &#123; -webkit-transform: rotateZ(-360deg); &#125;&#125;@-moz-keyframes play &#123; 0% &#123; -moz-transform: rotateZ(0deg); &#125; 100% &#123; -moz-transform: rotateZ(-360deg); &#125;&#125;@keyframes play &#123; 0% &#123; transform: rotateZ(0deg); &#125; 100% &#123; transform: rotateZ(-360deg); &#125;&#125; 添加相册我的相册。原理很简单，就是建立一个github仓库存储用于存储图片，然后将每个图片的路径保存到一个json文件里，在hexo博客中解析这个json文件，渲染成html页面后就可以在显示图片了。当然这里肯定要有页面的样式和图片的裁剪压缩，原理简单，实际操作起来有一些坑，我并不懂css样式，还是要感谢litten提供的方法。 首先在github上新建一个仓库，命名Blog_Album 然后本地新建一个文件夹Blog_Album，进入文件夹新建两个子文件夹photos，min_photos，然后下载这两个文件ImageProcess.py，tool.py到Blog_Album 记住之后要上传的相片就放到photos文件夹内。 在博客根目录运行 1hexo new photos 回到Blog_Album文件夹，将tools.py`中131行 123final_dict = &#123;&quot;list&quot;: list_info&#125; with open(&quot;D:/Blog/source/photos/data.json&quot;,&quot;w&quot;) as fp: json.dump(final_dict, fp) 将路径改为你的博客photos文件夹的相应位置 添加一些图片到Blog_Album的photos文件夹中 将本地Blog_Album与github仓库Blog_Album关联 运行tool.py脚本（因为脚本中有上传到github的函数，所以不用手动git push） 将本地Blog_Album上传到github仓库Blog_Album 回到yourblog\source\photos目录下，将index.md内容修改为 123456789101112131415161718192021222324252627---title: 我的相册date: 2017-12-29 22:32:22type: "photos"---&lt;link rel="stylesheet" href="./ins.css"&gt; &lt;link rel="stylesheet" href="./photoswipe.css"&gt; &lt;link rel="stylesheet" href="./default-skin/default-skin.css"&gt; &lt;div class="photos-btn-wrap"&gt; &lt;a class="photos-btn active" href="javascript:void(0)"&gt;Photos&lt;/a&gt;&lt;/div&gt;&lt;div class="instagram itemscope"&gt; &lt;a href="http://yanss.top" target="_blank" class="open-ins"&gt;图片正在加载中…&lt;/a&gt;&lt;/div&gt; &lt;script&gt; (function() &#123; var loadScript = function(path) &#123; var $script = document.createElement('script') document.getElementsByTagName('body')[0].appendChild($script) $script.setAttribute('src', path) &#125; setTimeout(function() &#123; loadScript('./ins.js') &#125;, 0) &#125;)()&lt;/script&gt; 第13行链接为自己的博客url 然后在photos文件夹下添加这些内容 文件这里下载，data.json是图片的数据信息，运行python脚本后会生成 ins.js中的114行render()函数需要修改这两个变量 12var minSrc = 'https://raw.githubusercontent.com/fakeYanss/Blog_Album/master/min_photos/' + data.link[i];var src = 'https://raw.githubusercontent.com/fakeYanss/Blog_Album/master/photos/' + data.link[i]; 如果你的仓库名和我相同，只用把这里的fakeYanss改为你自己的github name即可 在yourBlog/themes/next/source/js/src下加入两个js文件photoswipe.min.js 和photoswipe-ui-default.min.js 在yourBlog/themes/next/layout/_scripts/pages/post-details.swig中添加 12&lt;script type="text/javascript" src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt;&lt;script type="text/javascript" src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe-ui-default.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt; 在yourBlog/themes/next/layout/_layout.swig中 head内插入 12&lt;script src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt;&lt;script src="&#123;&#123; url_for(theme.js) &#125;&#125;/src/photoswipe-ui-default.min.js?v=&#123;&#123; theme.version &#125;&#125;"&gt;&lt;/script&gt; body内插入 1234567891011121314151617181920212223242526272829303132333435363738394041&#123;% if page.type === "photos" %&#125;&lt;!-- Root element of PhotoSwipe. Must have class pswp. --&gt;&lt;div class="pswp" tabindex="-1" role="dialog" aria-hidden="true"&gt; &lt;div class="pswp__bg"&gt;&lt;/div&gt; &lt;div class="pswp__scroll-wrap"&gt; &lt;div class="pswp__container"&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;div class="pswp__item"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="pswp__ui pswp__ui--hidden"&gt; &lt;div class="pswp__top-bar"&gt; &lt;div class="pswp__counter"&gt;&lt;/div&gt; &lt;button class="pswp__button pswp__button--close" title="Close (Esc)"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--share" title="Share"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--fs" title="Toggle fullscreen"&gt;&lt;/button&gt; &lt;button class="pswp__button pswp__button--zoom" title="Zoom in/out"&gt;&lt;/button&gt; &lt;!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR --&gt; &lt;!-- element will get class pswp__preloader--active when preloader is running --&gt; &lt;div class="pswp__preloader"&gt; &lt;div class="pswp__preloader__icn"&gt; &lt;div class="pswp__preloader__cut"&gt; &lt;div class="pswp__preloader__donut"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"&gt; &lt;div class="pswp__share-tooltip"&gt;&lt;/div&gt; &lt;/div&gt; &lt;button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)"&gt; &lt;/button&gt; &lt;button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"&gt; &lt;/button&gt; &lt;div class="pswp__caption"&gt; &lt;div class="pswp__caption__center"&gt;&lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&#123;% endif %&#125; 重新生成博客内容即可看到相册内容。 如果py脚本不能运行，先安装python环境，再安装Pillow库pip install Pillow 相册图片的命名请遵循yyyy-mm-dd_abc.efg格式 最后的不足是，相片的裁剪算法不算好，比如会这样，还有的会这样 next主题源码是不支持相册的，如果有不懂的地方，可以去查一下yilia主题的issue，然后再来问我 2018.1.20修改：由于从github仓库读取图片，在html页面中会发生ios手机竖持拍照的照片90度旋转问题（图片的EXIF的orientaion信息在裁剪压缩后发生改变），找了一些办法都没效果，所以将相片源仓库转移到七牛云，利用七牛云外链后加上?imageMogr2/auto-orient的方式，可以将照片正常角度显示。 添加Gitment评论原本是用的livere评论，后来总是加载速度太慢，上了梯子也一样，索性改成了Gitment评论。 感谢作者imsun，Gitment评论源自github仓库的issue，所以在将博客地址关联了某个github仓库后，在博客下评论其实就是在对应仓库的issue中评论，这创意真是太好了。 我之前的next主题一直是5.1.0版本，本来是想在主题中添加gitment的js和css文件，结果没成功。然后在next的官方文档中看到已经发行到5.1.4了，而且已经集成了gitment评论。这下可方便，干脆直接升级了next主题，然后就改config文件就好啦。 以前主题中配置了一些设置项，时间久了还忘了改了哪些文件！！！所以升级版本很痛苦，用的Sublime的一个插件Sublimerge，可以对比两个文件的代码差异，就是这样就像git pull操作之后改动时一样，这样子把每个有可能改过的文件都对比了一遍，然后升级到了5.1.4，发现集成了很多新功能，其他的有时间再试吧，这里就只说gitment。 首先在这里注册一个OAuth Application，Homepage URL和Authorization callback URL填写博客首页地址，也就是站点配置文件中的url，其他随意填写即可。 然后会得到一个Client ID和Client Secret，把这两个值填到主题配置文件的gitment对应位置12345678910111213gitment: enable: true mint: true # gitment仓库有两个，这里填true是引用第一个，false引用第二个，具体在layout下的conment文件中可以找到 count: true # 评论计数 lazy: false # 如果要点击按钮再显示评论就填true cleanly: true # 隐藏底部信息 language: # Force language, or auto switch by theme github_user: 填github ID github_repo: 填保存issue的仓库名，一般就用博客发布的仓库名 client_id: 刚才的值 client_secret: 刚才的值 proxy_gateway: # 设置代理，不用填 redirect_protocol: # 没搞懂，不用填 然后重新部署博客(本地调试是没用的，因为url不同)，再打开博客，这时候需要在每一个有评论的页面上使用自己的github长航登录并初始化一遍评论，之后就不用了。文章多的话会有点麻烦，不知道gitment作者有没有做好自动初始化？好像查到了这个，不过我还没试过。这里是成功的样子 但是，这个鼠标放上有两条横线什么鬼啊！！！ 还有这里头像下面为什么有一条横线！！！！ 强迫症忍不了，查看了gitment的css定义，没发现什么问题啊，然后在浏览器中调试，发现了这个123456a&#123; color: #555; border-bottom: 1px solid #999; text-decoration: none; word-wrap: break-word;&#125; 这里的border-bottom: 1px solid #999就是a标签下有一条横线的意思，但是这个属性是主题的属性main.css啊，显然是不能改的，于是只有在themes\next\source\css\_common\components\third-party\gitment.styl下改动了，在这里可以重写前面定义的属性，我是这样改的，在最后面加上123456789101112.gitment-comment-main a&#123; color: #555; border-bottom: none; text-decoration: none; word-wrap: break-word;&#125;.gitment-editor-avatar&#123; color: #555; border-bottom: none; text-decoration: none; word-wrap: break-word;&#125; 第一个就是修改的ID下的横线，显示为none就好了；第二个是修改编辑框头像下的横线，也是显示为none。 这样，算是完成了Gitment的配置了。 设置自定义页面不显示Sidebar主题配置文件中是这样的1234567toc: enable: true number: true wrap: falsesidebar: position: left display: post 讲道理这样就是是没有问题的，但是我发现自定义的页面里如果写了太多的#或者&lt;h1&gt;，就会被识别为post类型而不是page，也就是博客文章，会被自动加载目录，这就很蛋疼了不是，毕竟有的页面不想要目录啊尴尬！！！ 一般这种样式问题都在layout文件夹中找原因。 在themes\next\layout\_macro\sidebar.swig，找到开头的12345678&#123;% macro render(is_post) %&#125; &lt;div class="sidebar-toggle"&gt; &lt;div class="sidebar-toggle-line-wrap"&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-first"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-middle"&gt;&lt;/span&gt; &lt;span class="sidebar-toggle-line sidebar-toggle-line-last"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; 在下面加上1&#123;% if page.toc and theme.toc.enable %&#125; 然后在倒数第二行加上1&#123;% endif %&#125; 发现这样修改有bug，重新改。在themes\next\layout\_macro\sidebar.swig找到这一句1&#123;% set display_toc = is_post and theme.toc.enable or is_page and theme.toc.enable %&#125; 改为1&#123;% set display_toc = is_post and theme.toc.enable or is_page and page.toc or is_page and theme.toc.enable and page.toc %&#125; 其实就是多加一个判断，判断页面的开头有没有toc属性 最后，在需要有sidebar目录的文章前加上toc: true即可。 为了以后的方便，可以在scaffolds\post.md中加上toc: true。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java垃圾回收浅析]]></title>
    <url>%2Farchives%2F52a381d.html</url>
    <content type="text"><![CDATA[GC的三种收集方法标记清除标记清除算法是最基础的收集算法，其他收集算法都是基于这种思想。 标记清除算法分为“标记”和“清除”两个阶段：首先标记出需要回收的对象，标记完成之后统一清除对象。 主要缺点： 效率问题，标记和清除过程效率不高 。 空间问题，标记清除之后会产生大量不连续的内存碎片。 标记整理标记整理，主要用于回收老年代。 标记操作和“标记-清除”算法一致，后续操作不只是直接清理对象，而是在清理无用对象完成后让所有存活的对象都向一端移动，并更新引用其对象的指针。 主要缺点：在标记-清除的基础上还需进行对象的移动，成本相对较高，好处则是不会产生内存碎片。 复制算法复制算法，主要用于回收新生代。 它将可用内存容量划分为大小相等的两块，每次只使用其中的一块。当这一块用完之后，就将还存活的对象复制到另外一块上面，然后在把已使用过的内存空间一次理掉。这样使得每次都是对其中的一块进行内存回收，不会产生碎片等情况，只要移动堆订的指针，按顺序分配内存即可，实现简单，运行高效。 主要缺点：内存缩小为原来的一半。 分代的垃圾回收策略分代的垃圾回收策略是基于这样一个事实：不同的对象的生命周期是不一样的。因此，不同生命周期的对象可以采取不同的回收算法，以便提高回收效率。 年轻代（Young Generation） 所有新生成的对象首先都是放在年轻代的。年轻代的目标就是尽可能快速的收集掉那些生命周期短的对象。 新生代内存按照8:1:1的比例分为一个eden区和两个survivor(survivor0,survivor1)区。一个Eden区，两个 Survivor区(一般而言)。大部分对象在Eden区中生成。 回收时先将eden区存活对象复制到一个survivor0区，然后清空eden区； 当这个survivor0区也存放满了时，则将eden区和survivor0区存活对象复制到另一个survivor1区，然后清空eden和这个survivor0区； 此时survivor0区是空的，然后将survivor0区和survivor1区交换，即保持survivor1区为空。 如此往复循环。 当survivor1区不足以存放 eden和survivor0的存活对象时，就将存活对象直接存放到老年代。若是老年代也满了就会触发一次Full GC，也就是新生代、老年代都进行回收 新生代发生的GC也叫做Minor GC，MinorGC发生频率比较高(不一定等Eden区满了才触发) 年老代（Old Generation） 在年轻代中经历了N次垃圾回收后仍然存活的对象，就会被放到年老代中。因此，可以认为年老代中存放的都是一些生命周期较长的对象。 内存比新生代也大很多(大概比例是1:2)，当老年代内存满时触发Major GC即Full GC，Full GC发生频率比较低，老年代对象存活时间比较长，存活率标记高。 持久代（Permanent Generation）用于存放静态文件，如Java类、方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如Hibernate 等，在这种时候需要设置一个比较大的持久代空间来存放这些运行过程中新增的类。 Oracle JDK8的HotSpot VM去掉“持久代”，以“元数据区”（Metaspace）替代之。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018年应届硕士毕业生如何拿到知名互联网公司深度学习offer？]]></title>
    <url>%2Farchives%2F70b5d312.html</url>
    <content type="text"><![CDATA[本文作者：熊风 HKUST CS MPhil , 计算机视觉、机器学习 543 人赞同了该回答 最近投了一堆机器学习/深度学习/计算机视觉方向的公司，分享一下自己的经验，希望对大家有帮助。 个人背景： 华科本科 + 港科大硕士（MPhil） 拿到的offer有腾讯优图，阿里AI lab，今日头条，滴滴研究院，商汤科技，旷视（face++），大疆，快手。绝大部分是ssp（super special），给到了普通硕士能给到的最高档，例如滴滴的offer是滴滴新锐，腾讯的offer是技术大咖等等。这些offer大部分待遇在40W-50W之间，个别公司算上期权能到60W 写在前面的话这个回答的适用对象主要还是本科和硕士。PhD找工作的套路跟硕士还是很不一样的，所以这个回答的经验对于手握几篇一作顶会的PhD大神并没啥参考意义。 我也和我们实验室几个找工作的PhD学长学姐聊过，他们的面试主要是讲自己的research，有的甚至就是去公司给个talk，跟本科硕士的校招流程完全不同。现在也是AI方向PhD的黄金时代，没毕业就被各大公司主动联系，待遇也比我这种硕士高很多很多。 一. 整体建议一定要找内推。内推一般有两种，第一种力度比较弱，在公司的内推系统上填一下你的名字，加快一下招聘流程；第二种力度比较强，直接把简历送到部门负责人手上。个人比较建议第二种，会省事很多。 原因如下： （1）现在做机器学习的人实在太多了，在不找内推的情况下，流程会特别特别慢。即使你的简历比较优秀，也可能淹没在茫茫大海中，不一定能被懂行的人看到。 （2）现在很多公司的笔试其实挺有难度的，就算是大神也有翻车的可能性。 （3）对于大公司而言，即使通过了简历筛选、笔试那一关，你也很难保证你的简历被合适的部门挑中。很可能过关斩将后，发现给你安排的面试官并不是太对口。尤其是深度学习这样比较新的领域，一般部门的面试官多半也是近期自学的，对这个也是一知半解。所以如果是想去BAT这些大公司里面专门做AI的部门，按照正常校招流程走是不合适的，一定要找到那些部门的员工内推。在我看来，如果是跪在简历筛选、笔试这些上面，连面试官都没见到，就实在太可惜了。为了避免这一点，请认真找内推。最好能联系到你想去的公司部门里的负责人，直接安排面试。 二. 面试经验面试遇到的题目，可以分为几个大类： （1）代码题（leetcode类型）主要考察数据结构和基础算法，以及代码基本功。 虽然这部分跟机器学习，深度学习关系不大，但也是面试的重中之重。基本每家公司的面试都问了大量的算法题和代码题，即使是商汤、face++这样的深度学习公司，考察这部分的时间也占到了我很多轮面试的60%甚至70%以上。我去face++面试的时候，面试官是residual net，shuffle net的作者；但他们的面试中，写代码题依旧是主要的部分。 大部分题目都不难，基本是leetcode medium的难度。但是要求在现场白板编程，思路要流畅，能做到一次性Bug-free. 并且，一般都是要给出时间复杂度和空间复杂度最优的做法。对于少数难度很大的题，也不要慌张。一般也不会一点思路也没有，尽力给面试官展现自己的思考过程。面试官也会引导你，给一点小提示，沿着提示把题目慢慢做出来也是可以通过面试的。 以下是我所遇到的一些需要当场写出完整代码的题目： 二分查找。分别实现C++中的lower_bound和upper_bound. 排序。 手写快速排序，归并排序，堆排序都被问到过。 给你一个数组，求这个数组的最大子段积 时间复杂度可以到O(n) 给你一个数组，在这个数组中找出不重合的两段，让这两段的字段和的差的绝对值最大。 时间复杂度可以到O(n) 给你一个数组，求一个k值，使得前k个数的方差 + 后面n-k个数的方差最小 时间复杂度可以到O(n) 给你一个只由0和1组成的字符串，找一个最长的子串，要求这个子串里面0和1的数目相等。 时间复杂度可以到O(n) 给你一个数组以及一个数K， 从这个数组里面选择三个数，使得三个数的和小于等于K， 问有多少种选择的方法？ 时间复杂度可以到O(n^2) 给你一个只由0和1组成的矩阵，找出一个最大的子矩阵，要求这个子矩阵是方阵，并且这个子矩阵的所有元素为1 时间复杂度可以到O(n^2) 求一个字符串的最长回文子串 时间复杂度可以到O(n) (Manacher算法) 在一个数轴上移动，初始在0点，现在要到给定的某一个x点， 每一步有三种选择，坐标加1，坐标减1，坐标乘以2，请问最少需要多少步从0点到x点。 给你一个集合，输出这个集合的所有子集。 给你一个长度为n的数组，以及一个k值（k &lt; n) 求出这个数组中每k个相邻元素里面的最大值。其实也就是一个一维的max pooling 时间复杂度可以到O(n) 写一个程序，在单位球面上随机取点，也就是说保证随机取到的点是均匀的。 给你一个长度为n的字符串s，以及m个短串（每个短串的长度小于10）， 每个字符串都是基因序列，也就是说只含有A,T,C,G这四个字母。在字符串中找出所有可以和任何一个短串模糊匹配的子串。模糊匹配的定义，两个字符串长度相等，并且至多有两个字符不一样，那么我们就可以说这两个字符串是模糊匹配的。 其它一些描述很复杂的题这里就不列了。 （2）数学题或者”智力”题不会涉及特别高深的数学知识，一般就是工科数学（微积分，概率论，线性代数）和一些组合数学的问题。 下面是我在面试中被问到过的问题： 如果一个女生说她集齐了十二个星座的前男友，她前男友数量的期望是多少？ ps：这道题在知乎上有广泛的讨论，作为知乎重度用户我也看到过。如果一个女生说，她集齐了十二个星座的前男友，我们应该如何估计她前男友的数量？ 两个人玩游戏。有n堆石头，每堆分别有a1, a2, a3…. an个石头，每次一个游戏者可以从任意一堆石头里拿走至少一个石头，也可以整堆拿走，但不能从多堆石头里面拿。无法拿石头的游戏者输，请问这个游戏是否有先手必胜或者后手必胜的策略？ 如果有，请说出这个策略，并证明这个策略能保证必胜。 一个一维数轴，起始点在原点。每次向左或者向右走一步，概率都是0.5. 请问回到原点的步数期望是多少？ 一条长度为1的线段，随机剪两刀，求有一根大于0.5的概率。 讲一下你理解的矩阵的秩。低秩矩阵有什么特点？ 在图像处理领域，这些特点有什么应用？ 讲一下你理解的特征值和特征向量。 为什么负梯度方向是使函数值下降最快的方向？简单数学推导一下 （3）机器学习基础这部分建议参考周志华老师的《机器学习》。 下面是我在面试中被问到过的问题： https://www.nowcoder.com/discuss/65323 逻辑回归和线性回归对比有什么优点？ 逻辑回归可以处理非线性问题吗？ 分类问题有哪些评价指标？每种的适用场景。 讲一下正则化，L1和L2正则化各自的特点和适用场景。 讲一下常用的损失函数以及各自的适用场景。 讲一下决策树和随机森林 讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系 手推softmax loss公式 讲一下SVM, SVM与LR有什么联系。 讲一下PCA的步骤。PCA和SVD的区别和联系 讲一下ensemble 偏差和方差的区别。ensemble的方法中哪些是降低偏差，哪些是降低方差？ …… 这部分问得太琐碎了，我能记起来的问题就这么多了。我的感觉，这部分问题大多数不是问得很深，所以不至于被问得哑口无言，总有得扯；但是要想给出一个特别深刻的回答，还是需要对机器学习的基础算法了解比较透彻。 （4）深度学习基础这部分的准备，我推荐花书（Bengio的Deep learning）和 @魏秀参 《解析卷积神经网络-深度学习实践手册》 手推BP 手推RNN和LSTM结构 LSTM中每个gate的作用是什么，为什么跟RNN比起来，LSTM可以防止梯度消失 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？ 梯度消失和梯度爆炸的原因是什么？ 有哪些解决方法？ CNN和RNN的梯度消失是一样的吗？ 有哪些防止过拟合的方法？ 讲一下激活函数sigmoid，tanh，relu. 各自的优点和适用场景？ relu的负半轴导数都是0，这部分产生的梯度消失怎么办？ batch size对收敛速度的影响。 讲一下batch normalization CNN做卷积运算的复杂度。如果一个CNN网络的输入channel数目和卷积核数目都减半，总的计算量变为原来的多少？ 讲一下AlexNet的具体结构，每层的作用 讲一下你怎么理解dropout，分别从bagging和正则化的角度 data augmentation有哪些技巧？ 讲一下你了解的优化方法，sgd, momentum, rmsprop, adam的区别和联系 如果训练的神经网络不收敛，可能有哪些原因？ 说一下你理解的卷积核， 1x1的卷积核有什么作用？ …….. 同上，这部分的很多问题也是每个人都或多或少能回答一点，但要答得很好还是需要功底的。 （5）科研上的开放性问题这部分的问题没有固定答案，也没法很好地针对性准备。功在平时，多读paper多思考，注意培养自己的insight和intuition 下面是我在面试中被问到过的问题： 选一个计算机视觉、深度学习、机器学习的子领域，讲一下这个领域的发展脉络，重点讲出各种新方法提出时的motivation，以及谈谈这个领域以后会怎么发展。 讲一下你最近看的印象比较深的paper 讲一下经典的几种网络结构， AlexNet， VGG，GoogleNet， Residual Net等等，它们各自最重要的contribution 你看过最近很火的XXX paper吗? 你对这个有什么看法？ …… （6） 编程语言、操作系统等方面的一些问题。 C++， Python， 操作系统，Linux命令等等。这部分问得比较少，但还是有的，不具体列了 （7）针对简历里项目/论文 / 实习的一些问题。这部分因人而异，我个人的对大家也没参考价值，也不列了。 三. 平时应该怎么准备在大多数情况下，你能拿到什么样的offer，其实已经被你的简历决定了。如果平时没有积累相关的经历和成果，很难只靠面试表现就拿到非常好的offer。所以建议大家平时积累算法岗所看重的一些干货。 下面几点算是找AI相关工作的加分项： （1）一作的顶级会议论文 （2）AI领域知名公司的实习经历（长期实习更好） （3）相关方向有含金量的项目经历 （4）计算机视觉竞赛，数据挖掘竞赛的获奖或者优秀名次。现在这类竞赛太多了，就不具体列了。 （5）程序设计竞赛的获奖（例如OI/ACM/topcoder之类的） 当然，名校、高GPA这些是针对所有领域都有用的加分项，同样也是适用于这个领域的。 所以我的建议就是，如果自己所在的实验室很厉害，资源丰富，就专心做科研，发paper； 如果所在的实验室一般，没法产出相关的优秀成果，可以考虑自己做比赛和找实习。有一份知名公司的实习经历之后，找工作难度会下降很多。 最后，祝有志于AI这个领域的人都能拿到满意的offer. 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap源码剖析]]></title>
    <url>%2Farchives%2Fa73d4680.html</url>
    <content type="text"><![CDATA[本文转发为了方便查看，原文地址 HashMap简介HashMap是基于哈希表实现的，每一个元素都是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阈值）时，同样会自动增长。 HashMap是非线程安全的，只是用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 HashMap实现了Serializable接口，因此它支持序列化，实现了Cloneable接口，能被克隆。 HashMap源码剖析HashMap的源码如下（加入了比较详细的注释）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package java.util; import java.io.*; public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 默认的初始容量（容量为HashMap中槽的数目）是16，且实际容量必须是2的整数次幂。 static final int DEFAULT_INITIAL_CAPACITY = 16; // 最大容量（必须是2的幂且小于2的30次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认加载因子为0.75 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 存储数据的Entry数组，长度是2的幂。 // HashMap采用链表法解决冲突，每一个Entry本质上是一个单向链表 transient Entry[] table; // HashMap的底层数组中已用槽的数量 transient int size; // HashMap的阈值，用于判断是否需要调整HashMap的容量（threshold = 容量*加载因子） int threshold; // 加载因子实际大小 final float loadFactor; // HashMap被改变的次数 transient volatile int modCount; // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); // HashMap的最大容量只能是MAXIMUM_CAPACITY if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //加载因此不能小于0 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 找出“大于initialCapacity”的最小的2的幂 int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; // 设置“加载因子” this.loadFactor = loadFactor; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(capacity * loadFactor); // 创建Entry数组，用来保存数据 table = new Entry[capacity]; init(); &#125; // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 默认构造函数。 public HashMap() &#123; // 设置“加载因子”为默认加载因子0.75 this.loadFactor = DEFAULT_LOAD_FACTOR; // 设置“HashMap阈值”，当HashMap中存储数据的数量达到threshold时，就需要将HashMap的容量加倍。 threshold = (int)(DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR); // 创建Entry数组，用来保存数据 table = new Entry[DEFAULT_INITIAL_CAPACITY]; init(); &#125; // 包含“子Map”的构造函数 public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); // 将m中的全部元素逐个添加到HashMap中 putAllForCreate(m); &#125; //求hash值的方法，重新计算hash值 static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; // 返回h在数组中的索引值，这里用&amp;代替取模，旨在提升效率 // h &amp; (length-1)保证返回值的小于length static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; public int size() &#123; return size; &#125; public boolean isEmpty() &#123; return size == 0; &#125; // 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; //判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; //没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; // HashMap是否包含key public boolean containsKey(Object key) &#123; return getEntry(key) != null; &#125; // 返回“键为key”的键值对 final Entry&lt;K,V&gt; getEntry(Object key) &#123; // 获取哈希值 // HashMap将“key为null”的元素存储在table[0]位置，“key不为null”的则调用hash()计算哈希值 int hash = (key == null) ? 0 : hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null; &#125; // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; // putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; // 创建HashMap对应的“添加方法”， // 它和put()不同。putForCreate()是内部方法，它被构造函数等调用，用来创建HashMap // 而put()是对外提供的往HashMap中添加元素的方法。 private void putForCreate(K key, V value) &#123; int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); // 若该HashMap表中存在“键值等于key”的元素，则替换该元素的value值 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; e.value = value; return; &#125; &#125; // 若该HashMap表中不存在“键值等于key”的元素，则将该key-value添加到HashMap中 createEntry(hash, key, value, i); &#125; // 将“m”中的全部元素都添加到HashMap中。 // 该方法被内部的构造HashMap的方法所调用。 private void putAllForCreate(Map&lt;? extends K, ? extends V&gt; m) &#123; // 利用迭代器将元素逐个添加到HashMap中 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); putForCreate(e.getKey(), e.getValue()); &#125; &#125; // 重新调整HashMap的大小，newCapacity是调整后的容量 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; //如果就容量已经达到了最大值，则不能再扩容，直接返回 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; // 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; // 将"m"的全部元素都添加到HashMap中 public void putAll(Map&lt;? extends K, ? extends V&gt; m) &#123; // 有效性判断 int numKeysToBeAdded = m.size(); if (numKeysToBeAdded == 0) return; // 计算容量是否足够， // 若“当前阀值容量 &lt; 需要的容量”，则将容量x2。 if (numKeysToBeAdded &gt; threshold) &#123; int targetCapacity = (int)(numKeysToBeAdded / loadFactor + 1); if (targetCapacity &gt; MAXIMUM_CAPACITY) targetCapacity = MAXIMUM_CAPACITY; int newCapacity = table.length; while (newCapacity &lt; targetCapacity) newCapacity &lt;&lt;= 1; if (newCapacity &gt; table.length) resize(newCapacity); &#125; // 通过迭代器，将“m”中的元素逐个添加到HashMap中。 for (Iterator&lt;? extends Map.Entry&lt;? extends K, ? extends V&gt;&gt; i = m.entrySet().iterator(); i.hasNext(); ) &#123; Map.Entry&lt;? extends K, ? extends V&gt; e = i.next(); put(e.getKey(), e.getValue()); &#125; &#125; // 删除“键为key”元素 public V remove(Object key) &#123; Entry&lt;K,V&gt; e = removeEntryForKey(key); return (e == null ? null : e.value); &#125; // 删除“键为key”的元素 final Entry&lt;K,V&gt; removeEntryForKey(Object key) &#123; // 获取哈希值。若key为null，则哈希值为0；否则调用hash()进行计算 int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中“键为key”的元素 // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 删除“键值对” final Entry&lt;K,V&gt; removeMapping(Object o) &#123; if (!(o instanceof Map.Entry)) return null; Map.Entry&lt;K,V&gt; entry = (Map.Entry&lt;K,V&gt;) o; Object key = entry.getKey(); int hash = (key == null) ? 0 : hash(key.hashCode()); int i = indexFor(hash, table.length); Entry&lt;K,V&gt; prev = table[i]; Entry&lt;K,V&gt; e = prev; // 删除链表中的“键值对e” // 本质是“删除单向链表中的节点” while (e != null) &#123; Entry&lt;K,V&gt; next = e.next; if (e.hash == hash &amp;&amp; e.equals(entry)) &#123; modCount++; size--; if (prev == e) table[i] = next; else prev.next = next; e.recordRemoval(this); return e; &#125; prev = e; e = next; &#125; return e; &#125; // 清空HashMap，将所有的元素设为null public void clear() &#123; modCount++; Entry[] tab = table; for (int i = 0; i &lt; tab.length; i++) tab[i] = null; size = 0; &#125; // 是否包含“值为value”的元素 public boolean containsValue(Object value) &#123; // 若“value为null”，则调用containsNullValue()查找 if (value == null) return containsNullValue(); // 若“value不为null”，则查找HashMap中是否有值为value的节点。 Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (value.equals(e.value)) return true; return false; &#125; // 是否包含null值 private boolean containsNullValue() &#123; Entry[] tab = table; for (int i = 0; i &lt; tab.length ; i++) for (Entry e = tab[i] ; e != null ; e = e.next) if (e.value == null) return true; return false; &#125; // 克隆一个HashMap，并返回Object对象 public Object clone() &#123; HashMap&lt;K,V&gt; result = null; try &#123; result = (HashMap&lt;K,V&gt;)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; // assert false; &#125; result.table = new Entry[table.length]; result.entrySet = null; result.modCount = 0; result.size = 0; result.init(); // 调用putAllForCreate()将全部元素添加到HashMap中 result.putAllForCreate(this); return result; &#125; // Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; // 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; // 创建Entry。将“key-value”插入指定位置。 void createEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); size++; &#125; // HashIterator是HashMap迭代器的抽象出来的父类，实现了公共了函数。 // 它包含“key迭代器(KeyIterator)”、“Value迭代器(ValueIterator)”和“Entry迭代器(EntryIterator)”3个子类。 private abstract class HashIterator&lt;E&gt; implements Iterator&lt;E&gt; &#123; // 下一个元素 Entry&lt;K,V&gt; next; // expectedModCount用于实现fast-fail机制。 int expectedModCount; // 当前索引 int index; // 当前元素 Entry&lt;K,V&gt; current; HashIterator() &#123; expectedModCount = modCount; if (size &gt; 0) &#123; // advance to first entry Entry[] t = table; // 将next指向table中第一个不为null的元素。 // 这里利用了index的初始值为0，从0开始依次向后遍历，直到找到不为null的元素就退出循环。 while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; &#125; public final boolean hasNext() &#123; return next != null; &#125; // 获取下一个元素 final Entry&lt;K,V&gt; nextEntry() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); Entry&lt;K,V&gt; e = next; if (e == null) throw new NoSuchElementException(); // 注意！！！ // 一个Entry就是一个单向链表 // 若该Entry的下一个节点不为空，就将next指向下一个节点; // 否则，将next指向下一个链表(也是下一个Entry)的不为null的节点。 if ((next = e.next) == null) &#123; Entry[] t = table; while (index &lt; t.length &amp;&amp; (next = t[index++]) == null) ; &#125; current = e; return e; &#125; // 删除当前元素 public void remove() &#123; if (current == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); Object k = current.key; current = null; HashMap.this.removeEntryForKey(k); expectedModCount = modCount; &#125; &#125; // value的迭代器 private final class ValueIterator extends HashIterator&lt;V&gt; &#123; public V next() &#123; return nextEntry().value; &#125; &#125; // key的迭代器 private final class KeyIterator extends HashIterator&lt;K&gt; &#123; public K next() &#123; return nextEntry().getKey(); &#125; &#125; // Entry的迭代器 private final class EntryIterator extends HashIterator&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Map.Entry&lt;K,V&gt; next() &#123; return nextEntry(); &#125; &#125; // 返回一个“key迭代器” Iterator&lt;K&gt; newKeyIterator() &#123; return new KeyIterator(); &#125; // 返回一个“value迭代器” Iterator&lt;V&gt; newValueIterator() &#123; return new ValueIterator(); &#125; // 返回一个“entry迭代器” Iterator&lt;Map.Entry&lt;K,V&gt;&gt; newEntryIterator() &#123; return new EntryIterator(); &#125; // HashMap的Entry对应的集合 private transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet = null; // 返回“key的集合”，实际上返回一个“KeySet对象” public Set&lt;K&gt; keySet() &#123; Set&lt;K&gt; ks = keySet; return (ks != null ? ks : (keySet = new KeySet())); &#125; // Key对应的集合 // KeySet继承于AbstractSet，说明该集合中没有重复的Key。 private final class KeySet extends AbstractSet&lt;K&gt; &#123; public Iterator&lt;K&gt; iterator() &#123; return newKeyIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsKey(o); &#125; public boolean remove(Object o) &#123; return HashMap.this.removeEntryForKey(o) != null; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“value集合”，实际上返回的是一个Values对象 public Collection&lt;V&gt; values() &#123; Collection&lt;V&gt; vs = values; return (vs != null ? vs : (values = new Values())); &#125; // “value集合” // Values继承于AbstractCollection，不同于“KeySet继承于AbstractSet”， // Values中的元素能够重复。因为不同的key可以指向相同的value。 private final class Values extends AbstractCollection&lt;V&gt; &#123; public Iterator&lt;V&gt; iterator() &#123; return newValueIterator(); &#125; public int size() &#123; return size; &#125; public boolean contains(Object o) &#123; return containsValue(o); &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // 返回“HashMap的Entry集合” public Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet() &#123; return entrySet0(); &#125; // 返回“HashMap的Entry集合”，它实际是返回一个EntrySet对象 private Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet0() &#123; Set&lt;Map.Entry&lt;K,V&gt;&gt; es = entrySet; return es != null ? es : (entrySet = new EntrySet()); &#125; // EntrySet对应的集合 // EntrySet继承于AbstractSet，说明该集合中没有重复的EntrySet。 private final class EntrySet extends AbstractSet&lt;Map.Entry&lt;K,V&gt;&gt; &#123; public Iterator&lt;Map.Entry&lt;K,V&gt;&gt; iterator() &#123; return newEntryIterator(); &#125; public boolean contains(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry&lt;K,V&gt; e = (Map.Entry&lt;K,V&gt;) o; Entry&lt;K,V&gt; candidate = getEntry(e.getKey()); return candidate != null &amp;&amp; candidate.equals(e); &#125; public boolean remove(Object o) &#123; return removeMapping(o) != null; &#125; public int size() &#123; return size; &#125; public void clear() &#123; HashMap.this.clear(); &#125; &#125; // java.io.Serializable的写入函数 // 将HashMap的“总的容量，实际容量，所有的Entry”都写入到输出流中 private void writeObject(java.io.ObjectOutputStream s) throws IOException &#123; Iterator&lt;Map.Entry&lt;K,V&gt;&gt; i = (size &gt; 0) ? entrySet0().iterator() : null; // Write out the threshold, loadfactor, and any hidden stuff s.defaultWriteObject(); // Write out number of buckets s.writeInt(table.length); // Write out size (number of Mappings) s.writeInt(size); // Write out keys and values (alternating) if (i != null) &#123; while (i.hasNext()) &#123; Map.Entry&lt;K,V&gt; e = i.next(); s.writeObject(e.getKey()); s.writeObject(e.getValue()); &#125; &#125; &#125; private static final long serialVersionUID = 362498820763181265L; // java.io.Serializable的读取函数：根据写入方式读出 // 将HashMap的“总的容量，实际容量，所有的Entry”依次读出 private void readObject(java.io.ObjectInputStream s) throws IOException, ClassNotFoundException &#123; // Read in the threshold, loadfactor, and any hidden stuff s.defaultReadObject(); // Read in number of buckets and allocate the bucket array; int numBuckets = s.readInt(); table = new Entry[numBuckets]; init(); // Give subclass a chance to do its thing. // Read in size (number of Mappings) int size = s.readInt(); // Read the keys and values, and put the mappings in the HashMap for (int i=0; i&lt;size; i++) &#123; K key = (K) s.readObject(); V value = (V) s.readObject(); putForCreate(key, value); &#125; &#125; // 返回“HashMap总的容量” int capacity() &#123; return table.length; &#125; // 返回“HashMap的加载因子” float loadFactor() &#123; return loadFactor; &#125; &#125; 几点总结首先要清楚HashMap的存储结构，如下图所示：(图) 图中，紫色部分即代表哈希表，也称为哈希数组，数组的每个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 首先看链表中节点的数据结构：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071// Entry是单向链表。 // 它是 “HashMap链式存储法”对应的链表。 // 它实现了Map.Entry 接口，即实现getKey(), getValue(), setValue(V value), equals(Object o), hashCode()这些函数 static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; // 指向下一个节点 Entry&lt;K,V&gt; next; final int hash; // 构造函数。 // 输入参数包括"哈希值(h)", "键(k)", "值(v)", "下一节点(n)" Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 判断两个Entry是否相等 // 若两个Entry的“key”和“value”都相等，则返回true。 // 否则，返回false public final boolean equals(Object o) &#123; if (!(o instanceof Map.Entry)) return false; Map.Entry e = (Map.Entry)o; Object k1 = getKey(); Object k2 = e.getKey(); if (k1 == k2 || (k1 != null &amp;&amp; k1.equals(k2))) &#123; Object v1 = getValue(); Object v2 = e.getValue(); if (v1 == v2 || (v1 != null &amp;&amp; v1.equals(v2))) return true; &#125; return false; &#125; // 实现hashCode() public final int hashCode() &#123; return (key==null ? 0 : key.hashCode()) ^ (value==null ? 0 : value.hashCode()); &#125; public final String toString() &#123; return getKey() + "=" + getValue(); &#125; // 当向HashMap中添加元素时，绘调用recordAccess()。 // 这里不做任何处理 void recordAccess(HashMap&lt;K,V&gt; m) &#123; &#125; // 当从HashMap中删除元素时，绘调用recordRemoval()。 // 这里不做任何处理 void recordRemoval(HashMap&lt;K,V&gt; m) &#123; &#125; &#125; 它的结构元素除了key、value、hash外，还有next，next指向下一个节点。另外，这里覆写了equals和hashCode方法来保证键值对的独一无二。 HashMap共有四个构造方法。构造方法中提到了两个很重要的参数：初始容量和加载因子。这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（从构造函数中可以看出，如果不指明，则默认为16），加载因子是哈希表在其容量自动增加之前可以达到多满的一种尺度，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表进行 resize 操作（即扩容）。 下面说下加载因子，如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），对空间造成严重浪费。如果我们在构造方法中不指定，则系统默认加载因子为0.75，这是一个比较理想的值，一般情况下我们是无需修改的。 另外，无论我们指定的容量为多少，构造方法都会将实际容量设为不小于指定容量的2的次方的一个数，且最大值不能超过2的30次方 HashMap中key和value都允许为null。put和get要重点分析下HashMap中用的最多的两个方法put和get。 get 先从比较简单的get方法着手，源码如下： 12345678910111213141516171819202122232425262728// 获取key对应的value public V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; /判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; 没找到则返回null return null; &#125; // 获取“key为null”的元素的值 // HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！ private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。 如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 put put方法稍微复杂些，代码如下： 12345678910111213141516171819202122232425 // 将“key-value”添加到HashMap中 public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; //将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 如果key为null，则将其添加到table[0]对应的链表中，putForNullKey的源码如下：123456789101112131415// putForNullKey()的作用是将“key为null”键值对添加到table[0]位置 private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 如果没有存在key为null的键值对，则直接题阿见到table[0]处! modCount++; addEntry(0, null, value, 0); return null; &#125; 如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，比将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到改单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下：1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。 void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K,V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K,V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length); &#125; 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。 两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 关于扩容。上面我们看到了扩容的方法，resize方法，它的源码如下： 12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位 void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int)(newCapacity * loadFactor); &#125; 很明显，是新建了一个HashMap的底层数组，而后调用transfer方法，将就HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。transfer方法的源码如下：123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中 void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K,V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125; &#125; 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap的时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 注意containsKey方法和containsValue方法。前者直接可以通过key的哈希值将搜索范围定位到指定索引对应的链表，而后者要对哈希数组的每个链表进行搜索。 求hash值和索引值我们重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。 计算哈希值的方法如下：1234static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125; 它只是一个数学公式，IDK这样设计对hash值的计算，自然有它的好处，至于为什么这样设计，我们这里不去追究，只要明白一点，用的位的操作使hash值的计算效率很高。 由hash值找到对应索引的方法如下：123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; 这个我们要重点说下，我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除法散列法），Hashtable中也是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，HashMap中则通过h&amp;(length-1)的方法来代替取模，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。首先，length为2的整数次幂的话，h&amp;(length-1)就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率；其次，length为2的整数次幂的话，为偶数，这样length-1为奇数，奇数的最后一位是1，这样便保证了h&amp;(length-1)的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀性，而如果length为奇数的话，很明显length-1为偶数，它的最后一位是0，这样h&amp;(length-1)的最后一位肯定为0，即只能为偶数，这样任何hash值都只会被散列到数组的偶数下标位置上，这便浪费了近一半的空间，因此，length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去北京]]></title>
    <url>%2Farchives%2Fd87f7e0c.html</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[css实现图片轮播]]></title>
    <url>%2Farchives%2F64ac0229.html</url>
    <content type="text"><![CDATA[本文转自知乎 由于css无法做到js一样的精准操控，所有某些效果是无法实现的，比如在轮播的同时支持用户左右滑动，所以使用css只能实现基本的效果。下面列出来的内容就是我们实现的： 在固定区域中，内部内容自行滑动切换形成播放的效果 当切换到最后一张内容时，会反向播放或者回到起点重播 每张内容会停留一段时间，让用户能够看清楚 内容可以点击/进行操作 dom结构搭建首先要有一个容器作为轮播图的容器，同时由于要实现滑动切换，所以内部需要有一个装载所有待切换内容的子容器 如果子容器中的内容是左右切换的，则需要将内容左右排列开 下面以轮播图片为例，上代码123456789&lt;div class="loop-wrap"&gt; &lt;div class="loop-images-container"&gt; &lt;img src="darksky.jpg" alt="" class="loop-image"&gt; &lt;img src="starsky.jpg" alt="" class="loop-image"&gt; &lt;img src="whiteland.jpg" alt="" class="loop-image"&gt; &lt;img src="darksky.jpg" alt="" class="loop-image"&gt; &lt;img src="starsky.jpg" alt="" class="loop-image"&gt; &lt;/div&gt;&lt;/div&gt; .loop-wrap 是主容器 .loop-images-container 是承载内部图片的子容器 .loop-image 是图片内容，如果需要显示其他内容，可以自定义 css实现静态效果轮播图内每一页内容的宽高应该相同，且等于主容器.loop-wrap宽高 .loop-images-container的宽高必然有一个大于外部主容器，overflow属性应该设置为hidden。那为什么不设置为auto呢？我不告诉你，你可以自己试试看(这里原因是auto属性会在内容超出时自动加载出容器的下拉条)1234567891011121314151617181920.loop-wrap &#123; position: relative; width: 500px; height: 300px; margin: 100px auto; overflow: hidden;&#125;.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; /* 横向排列 5张图片 宽度应为主容器5倍 */ height: 100%; font-size: 0;&#125;.loop-image&#123; width: 500px; height: 300px;&#125; css实现轮播效果轮播效果说到底就是一个动画效果，而通过css3的新属性 animation 我们就可以自定义一个动画来达到轮播图效果。下面先来了解一下 animation 这个属性12345678910/*animation: name duration timing-function delay iteration-count directionname: 动画名duration： 动画持续时间 设置为0则不执行timing-function：动画速度曲线delay：动画延迟开始时间 设置为0则不延迟iteration-count：动画循环次数 设置为infinite则无限次循环direction：是否应该轮流反向播放动画 normal 否 alternate 是*/ animation 的 name 值是动画名，动画名可以通过 @keyframes 创建自定义动画规则 分析动画要实现轮播，本质上是使内部承载内容的子容器 .loop-images-container 进行位移，从而使不同位置的内容一次展示在用户眼前 共有五张图片需要展示，如果轮播总耗时10s，那么每张图片应该有2s的时间(20%)，而每张图片耗时的构成是切换耗时+展示耗时，如果切换耗时500ms(5%)，展示耗时就应该是1500ms(15%) 于是这样改造css123456789101112131415161718192021222324252627282930313233.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; height: 100%; font-size: 0; transform: translate(0,0); /* 初始位置位移 */ animation: loop 10s linear infinite;&#125;/* 创建loop动画规则 *//* 轮播5张，总耗时10s，单张应为2s(20%) 单张切换动画耗时500ms，停留1500ms*/@keyframes loop &#123; 0% &#123;transform: translate(0,0);&#125; 15% &#123;transform: translate(0,0);&#125; /* 停留1500ms */ 20% &#123;transform: translate(-20%,0);&#125; /* 切换500ms 位移-20% */ 35% &#123;transform: translate(-20%,0);&#125; 40% &#123;transform: translate(-40%,0);&#125; 55% &#123;transform: translate(-40%,0);&#125; 60% &#123;transform: translate(-60%,0);&#125; 75% &#123;transform: translate(-60%,0);&#125; 80% &#123;transform: translate(-80%,0);&#125; 95% &#123;transform: translate(-80%,0);&#125; 100% &#123;transform: translate(0,0);&#125; /* 复位到第一张图片 */&#125; 这是一个方向的轮播效果，想要实现往返方向的轮播效果，小伙伴们可以试试direction的alternate，但是自定义动画规则的时间间隔也要重新计算了哦！ 以下是所有代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt;&lt;style type="text/css"&gt;.loop-wrap &#123; position: relative; width: 500px; height: 300px; margin: 100px auto; overflow: hidden;&#125;.loop-images-container&#123; position: absolute; left: 0; top: 0; width: 500%; /* 横向排列 5张图片 宽度应为主容器5倍 */ height: 100%; font-size: 0; transform: translate(0,0); /* 初始位置位移 */ animation: loop 10s linear infinite;&#125;.loop-image&#123; width: 500px; height: 300px;&#125;@keyframes loop &#123; 0% &#123;transform: translate(0,0);&#125; 15% &#123;transform: translate(0,0);&#125; /* 停留1500ms */ 20% &#123;transform: translate(-20%,0);&#125; /* 切换500ms 位移-20% */ 35% &#123;transform: translate(-20%,0);&#125; 40% &#123;transform: translate(-40%,0);&#125; 55% &#123;transform: translate(-40%,0);&#125; 60% &#123;transform: translate(-60%,0);&#125; 75% &#123;transform: translate(-60%,0);&#125; 80% &#123;transform: translate(-80%,0);&#125; 95% &#123;transform: translate(-80%,0);&#125; 100% &#123;transform: translate(0,0);&#125; /* 复位到第一张图片 */&#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="loop-wrap"&gt; &lt;div class="loop-images-container"&gt; &lt;img src="1.jpg" alt="" class="loop-image"&gt; &lt;img src="2.jpg" alt="" class="loop-image"&gt; &lt;img src="3.jpg" alt="" class="loop-image"&gt; &lt;img src="4.jpg" alt="" class="loop-image"&gt; &lt;img src="5.jpg" alt="" class="loop-image"&gt; &lt;/div&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 总结虽然css也能实现轮播效果，但是相对于js实现来说，功能性就弱了很多，无法控制暂停与播放，无法与用户产生交互，无法监听到状态的而变化等等，但是好处也很明显嘛！那就是简单。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Front-End</category>
      </categories>
      <tags>
        <tag>css3</tag>
        <tag>animation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从尾到头打印链表]]></title>
    <url>%2Farchives%2Fb724efa.html</url>
    <content type="text"><![CDATA[从尾到头打印链表题目描述输入一个链表，从尾到头打印链表每个节点的值。思路2：递归1234567891011import java.util.ArrayList;public class Solution &#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; if(listNode != null)&#123; this.printListFromTailToHead(listNode.next); arrayList.add(listNode.val); &#125; return arrayList; &#125;&#125; 思路2：利用栈1234567891011121314151617import java.util.Stack;import java.util.ArrayList;public class Solution &#123; public ArrayList&lt;Integer&gt; printListFromTailToHead(ListNode listNode) &#123; Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); while (listNode != null) &#123; stack.push(listNode.val); listNode = listNode.next; &#125; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); while (!stack.isEmpty()) &#123; list.add(stack.pop()); &#125; return list; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[替换空格]]></title>
    <url>%2Farchives%2Fcd96fb91.html</url>
    <content type="text"><![CDATA[替换空格题目描述请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路看到问题第一反应是用replaceAll()方法，但这样挺没意思的。所以自己写解决方法，这样的话，就有了两种思路。 从前往后替换，当遇到第一个空格时，要移动第一个空格后所有的字符一次；当遇到第二个空格时，要移动第二个空格后所有的字符一次；以此类推。 从后往前，先计算需要多少空间，然后从后往前移动，则每个字符只为移动一次，这样效率更高一点。 这里提供第二种方法的代码1234567891011121314151617181920212223public class Solution &#123; public String replaceSpace(StringBuffer str) &#123; int spacenum = 0;//spacenum为计算空格数 for(int i=0;i&lt;str.length();i++)&#123; if(str.charAt(i)==' ') spacenum++; &#125; int indexold = str.length()-1; //indexold为为替换前的str下标 int newlength = str.length() + spacenum*2;//计算空格转换成%20之后的str长度 int indexnew = newlength-1;//indexnew为把空格替换为%20后的str下标 str.setLength(newlength);//使str的长度扩大到转换成%20之后的长度,防止下标越界 for(;indexold&gt;=0 &amp;&amp; indexold&lt;newlength;--indexold)&#123; if(str.charAt(indexold) == ' ')&#123; // str.setCharAt(indexnew--, '0'); str.setCharAt(indexnew--, '2'); str.setCharAt(indexnew--, '%'); &#125;else&#123; str.setCharAt(indexnew--, str.charAt(indexold)); &#125; &#125; return str.toString(); &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二维数组中的查找]]></title>
    <url>%2Farchives%2F2aa4f70.html</url>
    <content type="text"><![CDATA[二维数组中的查找题目描述在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 解题思路利用二维数组由上到下，由左到右递增的规律，那么选取右上角或者左下角的元素a[row][col]与target进行比较，当target小于元素a[row][col]时，那么target必定在元素a所在行的左边,即col–；当target大于元素a[row][col]时，那么target必定在元素a所在列的下边,即row++；时间复杂度是O(2n)123456789101112131415public class Solution &#123; public boolean Find(int target, int [][] array) &#123; int row=0; int col=array[0].length-1; while(row&lt;=array.length-1&amp;&amp;col&gt;=0)&#123; if(target==array[row][col]) return true; else if(target&gt;array[row][col]) row++; else col--; &#125; return false; &#125;&#125; 另一种思路是把每一行看成有序递增的数组，利用二分查找，通过遍历每一行得到答案，时间复杂度是O(nlogn)1234567891011121314151617181920public class Solution &#123; public boolean Find(int [][] array,int target) &#123; for(int i=0;i&lt;array.length;i++)&#123; int low=0; int high=array[i].length-1; while(low&lt;=high)&#123; int mid=(low+high)/2; if(target&gt;array[i][mid]) low=mid+1; else if(target&lt;array[i][mid]) high=mid-1; else return true; &#125; &#125; return false; &#125;&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>剑指Offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java抽象类和接口]]></title>
    <url>%2Farchives%2Ff3ed7a5d.html</url>
    <content type="text"><![CDATA[InterruptedException中断故障(异常) Java抽象类和接口抽象类抽象类是用来描述抽象行为的，比如Animal，我们不知道Animal具体有会有什么样的行为，只有具体的动物类，如Dog，Cat才有具体的行为，才能够被实例化。抽象类是实现多态的一种机制，它可以包含具体方法（有具体实现的方法），也可以包含抽象方法，而继承它的子类必须实现这些方法，下面总结了一下抽象类的特性： 抽象类不能被实例化，但可以有构造函数 抽象方法必须由子类进行重写 只要包含一个抽象方法的类，就必须定义为抽象类，不管是否还包含其他方法 抽象类中可以包含具体的方法，也可以不包含抽象方法 抽象类可以包含普通成员变量，其访问类型可以任意 抽象类也可以包含静态成员变量，其访问类型可以任意 子类中的抽象方法不能与父类的抽象方法同名 abstract不能与private、static、final或native并列修饰同一个方法 下面通过一个实例类来说明抽象类的使用 123456789101112131415161718192021222324252627282930313233// 抽象类Animal，包含了一个抽象方法cryabstract class Animal&#123; public abstract void cry(); &#125;// 子类Dog继承的抽象类Animal，必须实现其抽象方法cryclass Dog extends Animal&#123; public void cry() &#123; System.out.println("Dog cry"); &#125;&#125;// 同样，子类Cat继承的抽象类Animal，必须实现其抽象方法cryclass Cat extends Animal&#123; public void cry() &#123; System.out.println("Cat cry"); &#125;&#125;public class Test &#123; public static void main(String[] args) &#123; Animal a1 = new Dog(); // 抽象类引用指向子类实例 Animal a2 = new Cat(); a1.cry(); a2.cry(); &#125;&#125; 输出结果如下：12Dog cryCat cry 由输出结果可以知道，使用a1,a2调用cry方法调用的是子类的cry方法，这是动态绑定，是实现多态的一种机制。 接口接口在Java当中是通过关键字interface来实现，接口不是类，所以也不能被实例化，接口是用来建立类与类之间的协议，它的提供的只是一种形式，而没有具体的实现。实现类实现(implements)接口，必须实现接口的全部方法 接口是抽象类的延伸，Java不允许多重继承（即不能有多个父类，只能有一个），但可以实现多个接口。在使用接口的过程中，就注意以下几个问题： 接口中不能有构造方法。 接口的所有方法自动被声明为public，而且只能为public，如果使用protected、private，会导致编译错误。 接口可以定义”成员变量”，而且会自动转为public final static，即常量，而且必须被显式初始化。 接口中的所有方法都是抽象方法，不能包含实现的方法，也不能包含静态方法 实现接口的非抽象类必须实现接口的所有方法，而抽象类不需要 不能使用new来实现化接口，但可以声明一个接口变量，它必须引用一个实现该接口的类的对象，可以使用instanceOf来判断一个类是否实现了某个接口，如if (object instanceOf ClassName){doSth()}; 在实现多接口的时候一定要注意方法名的重复 抽象类与接口的区别语法层次抽象类的定义，如下所示： 12345678// 抽象类中可以包含抽象方法与非抽象方法（必须给出实现）public abstract class Demo &#123; abstract void foo1(); void foo2()&#123; //实现 &#125;&#125; 接口的定义，如下所示： 123456interface Demo&#123; // 接口中的方法自动转为public abstract void foo1(); void foo2();&#125; 抽象类方式中，抽象类可以拥有任意范围的成员数据，同时也可以拥有自己的非抽象方法，但是接口方式中，它仅能够有静态、不能修改的成员数据（即final static，但是我们一般是不会在接口中使用成员数据），同时它所有的方法都必须是抽象的。在某种程度上来说，接口是抽象类的特殊化。 设计层次从设计的层面来看，我觉得抽象类与接口有如下几个不同点： 抽象层次不同。可以这样理解，抽象类是对类的抽象，接口是对行为的抽象。抽象类对是类整体进行抽象，包括属性、行为，而接口是对类局部（行为）进行抽象。 跨域不同。抽象类所跨域的是具有相似特点的类，而接口可以跨域不同的类。抽象类所体现的是一种继承关系，要想使得继承关系合理，父类和派生类之间必须 存在”is-a” 关系 ，即父类和派生类在概念本质上应该是相同的。对于接口则不然，并不要求接口的实现者和接口定义在概念本质上是一致的， 仅仅是实现了接口定义的契约而已。 总结 抽象类在java语言中所表示的是一种继承关系，一个子类只能存在一个父类，但是可以存在多个接口。 在抽象类中可以拥有自己的成员变量和非抽象类方法，但是接口中只能存在静态的不可变的成员数据（不过一般都不在接口中定义成员数据），而且它的所有方法都是抽象的。 抽象类和接口所反映的设计理念是不同的，抽象类所代表的是”is-a”的关系，而接口所代表的是”like-a”的关系。抽象类和接口是java语言中两种不同的抽象概念，他们的存在对多态提供了非常好的支持，虽然他们之间存在很大的相似性。但是对于他们的选择往往反应了您对问题域的理解。只有对问题域的本质有良好的理解，才能做出正确、合理的设计。 以上内容源自： 作者：AlvinL链接：http://www.jianshu.com/p/2b5a9bdcd25f來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 牛客网：优先选用接口，尽量少用抽象类 接口和抽象类都是继承树的上层，他们的共同点如下： 都是上层的抽象层。 都不能被实例化 都能包含抽象的方法，这些抽象的方法用于描述类具备的功能，但是不比提供具体的实现。 他们的区别如下： 在抽象类中可以写非抽象的方法，从而避免在子类中重复书写他们，这样可以提高代码的复用性，这是抽象类的优势；接口中只能有抽象的方法。 一个类只能继承一个直接父类，这个父类可以是具体的类也可是抽象类；但是一个类可以实现多个接口。 jsp中静态include和动态include 静态导入（include指令）通过file属性指定被包含的文件，并且file属性不支持任何表达式；动态导入（include动作）通过page属性指定被包含的文件，且page属性支持JSP表达式； 使用静态导入（include指令）时，被包含的文件内容会原封不动的插入到包含页中，然后JSP编译器再将合成后的文件最终编译成一个 Java文件；使用动态导入（include动作）包含文件时，当该标识被执行时，程序会将请求转发（不是请求重定向）到被包含的页面，并将执行结果输出 到浏览器中，然后返回包含页继续执行后面的代码。因为服务器执行的是多个文件，所以JSP编译器会分别对这些文件进行编译； 使用include静态指令包含文件时，由于被包含的文件最终会生成一个文件，所以在被包含、包含文件中不能有重名的变量或方法；而include动态包含文件时，由于每个文件是单独编译的，所以在被包含文件和包含文件中重名的变量和方法是不相冲突的。 静态导入是将被导入页面的代码完全融入，两个页面融合成一个整体Servlet，因此被导入页面甚至不需要是一个完整的页面；而动态导入则在Servlet中使用include方法来引入被导入页面的内容； 静态导入时被导入页面的编译指令会起作用；而动态导入时被导入页面的编译指令则失去作用，只是插入被导入页面的body内容。 一个以.java为后缀的源文件，只能有一个与文件名相同的public类，可以包含其他非public类（不考虑内部类）java中true ,false , null在java中不是关键字，也不是保留字，它们只是显式常量值，但是你在程序中不能使用它们作为标识符。其中const和goto是java的保留字。java中所有的关键字都是小写的，还有要注意true,false,null, friendly，sizeof不是java的关键字,但是你不能把它们作为java标识符用。 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>InterruptedException</tag>
        <tag>抽象类</tag>
        <tag>接口</tag>
        <tag>jsp include</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记事簿]]></title>
    <url>%2Farchives%2F61933d8.html</url>
    <content type="text"><![CDATA[点进来也没什么好看的，蛤蛤:) 20170814 一日之事始于午后，这就是我的工作写照。 20170901 云无心以出岫，鸟倦飞而知还。 20170904 萍水相逢，当知聚散无定；劳燕分飞，常念相会有时。 20171228 面上扫开十层甲，眉目才无可憎；胸中涤去数斗尘，语言方觉有味。 20180119 多谢爱奇艺收了我咯]]></content>
      <categories>
        <category>记事簿</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Coding+DaoCloud持续集成]]></title>
    <url>%2Farchives%2F93b24b01.html</url>
    <content type="text"><![CDATA[在上一篇博客中，按照步骤做完，就可以在本机发布博客到coding了，但这样我们只能用这一台电脑发博客，并且最大的问题是备份问题，如果这台电脑挂了，或者是误删文件，就很可能丢失了blog源文件。如果没有备份，就要gg了。 想要备份源文件，同时在多终端发布博客，最好的实现方式是找一个持续集成工具，在coding的帮助文档里有一些介绍，我试过一些，最后选择DaoCloud来集成coding仓库，原因还是免费。 这里的原理是，在coding的blog仓库中建立两个分支，分别是master和coding-pages。master分支存放blog源文件，即本地的hexo文件内容；coding-pages分支存放博客的全部静态页面，也就是blog\public文件夹中的内容。 操作的过程中，顺序是 git push 提交本地blog源文件到master分支 DaoCloud检测到master分支有提交内容，按照设置好的安装环境，生成博客，部署博客到coding-pages分支 这样在部署过一次之后，如果不再修改样式和主题等其他内容，只是提交博客，可以直接登录coding.net网站，将编辑好的md文件添加到master分支的blog/source/_post文件夹下，就完了。 如果要同步本地备份，也只用把master分支内容clone到本地就行了。 现在开始正式的操作。 创建新分支登录到coding官网中，可以继续保留上一篇博客中创建的仓库，先清空仓库（在项目设置里的仓库设置），然后在分支管理处新建分支coding-pages。 创建SSH Key文件夹由于上一篇博客《github+hexo搭建个人博客》中已经创建了SSH key，所以这里可以直接使用，在/blog/根目录下创建文件夹.daocloud，这里前面有.的文件夹不能直接创建，可以先直接创建aaa，再用命令行修改名字。此处右键打开git bash，1mv aaa .daocloud 然后把之前生成的SSH key复制到这个文件夹下1cp ~/.ssh/id_rsa* .daocloud/ 然后在.daocloud下新建文件ssh_config1touch ssh_config 打开ssh_config,输入12StrictHostKeyChecking noUserKnownHostsFile /dev/null 保存。 编辑Dockerfile在本地仓库blog根目录下新建文件名为Dockerfile的文件（没有后缀）,打开编辑内容如下，原因稍后再说： 1234567891011121314151617# DockerfileFROM node:slimMAINTAINER xxx &lt;xxx@xxx.com&gt;# 安装git、ssh等基本工具RUN apt-get update &amp;&amp; apt-get install -y git ssh-client ca-certificates --no-install-recommends &amp;&amp; rm -r /var/lib/apt/lists/*# 设置时区RUN echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone &amp;&amp; dpkg-reconfigure -f noninteractive tzdataRUN npm install -g cnpm --registry=https://registry.npm.taobao.org# 只安装Hexo命令行工具，其他依赖项根据项目package.json在持续集成过程中安装RUN cnpm install hexo-cli -g# install hexo serverRUN cnpm install hexo-serverEXPOSE 4000 这里只用替换上你自己的coding用户名和邮箱就行，别的不变。 修改本地_config.yml配置因为要把博客部署到coding-pages分支，所以要修改deploy参数，把_config.yml的deploy修改如下1234deploy: type: git repo: coding: git@git.coding.net:xxx/xxx.git,coding-pages coding后面替换为你自己的coding仓库地址，加上coding-pages分支。 DaoCloud创建项目和配置这里是因为DaoCloud系统有过升级改版，网上搜到的DaoCloud操作教程几乎都是去年12月以前的，所以有些对现在的版本不太适用，我自己借助旧的教程和部署AppVeyor的经验改动了一些，适用现在的DaoCloud。 下面先登录DaoCloud官网，用github或者coding账号直接登录，在个人设置中绑定github和coding，然后在控制台新建项目，项目名随便取。 选择成功构建后设置 latest 为镜像标签，然后点击镜像：ci-hexo（这里是我的名字，你就点你自己相应的），复制镜像地址，先记在一边等下要用。这里安利一个剪贴板管理软件Ditto。 然后打开流程定义，点击右侧通过 yaml 快捷编辑，打开后只一个脚本编辑页面，直接把以下内容复制进去： 123456789101112131415161718192021222324252627282930313233343536373839404142version: 3image: ubuntu:16.04stages:- build- test构建任务: stage: build job_type: image_build only: branches: - master build_dir: / cache: true dockerfile_path: /Dockerfile测试任务: stage: test job_type: test only: branches: - master pull_request: false before_script: - mkdir ~/.ssh - mv .daocloud/id_rsa ~/.ssh/id_rsa - mv .daocloud/ssh_config ~/.ssh/config - chmod 600 ~/.ssh/id_rsa - chmod 600 ~/.ssh/config - eval $(ssh-agent) - ssh-add ~/.ssh/id_rsa - rm -rf .daocloud - git config --global user.name &quot;xxxxx&quot; #这里填你的coding用户名 - git config --global user.email &quot;xxxxx@xxxxx.com&quot; #这里填你的coding邮箱 image: xxxxx:latest #这里填你的镜像url，不要覆盖latest install: - cnpm install - cnpm install --save hexo-generator-feed - cnpm install hexo-baidu-url-submit --save script: - hexo clean - hexo g - hexo d - rm -rf ~/.ssh/ 只用修改3处位置，其他的不要动，然后点击更新。 到了这里就快完成了，还差一点，在流程定义这里点击构建任务，修改触发条件为分支-master-执行任务，测试任务也修改触发条件为分支-master-执行任务。 git关联远程库和提交代码先在所有的配置就做完了，可以回到blog文件夹下，将本地仓库push到远程库就行了。现在介绍一下如何关联远程库，以及commit和push操作。 关联远程库，这里后面xxx是你的仓库url 1git remote add coding xxxxxx 添加代码到本地仓库和commit信息，这里xxx是你的commit信息，可以随便写 12git add .git commit -m &quot;xxx&quot; push到远程库的master分支 1git push -u coding master 如果提示有冲突，就把-u改为-f，反正仓库里都是自己的东西，force push也没什么大问题。 结束现在就完成了全部的操作，其实并不复杂，Dockerfile之预编译文件，放在仓库master分支中，每当仓库master分支有新的提交时，DaoCloud会监测到变化，由于触发条件的设置，就开始了构建任务和测试任务，构建任务阶段是编译Dockerfile文件中的脚本，其实大致意思很好懂，就是配置git，node和hexo环境。然后进行测试任务，就是进行刚才编辑的yaml编辑器中的脚本，其中有一段测试任务，逻辑就是安装必要的hexo包，然后复制coding仓库里的ssh密匙，用你配置的账号和邮箱，进行hexo操作，清除缓存，生成文件，部署到coding-pages分支，然后删除ssh密匙（安全性）。 虽然整个操作过程有暴露私有ssh key的风险，但说实话，git的初衷不就是为了分享代码吗，况且我们写的这些真实价值并没有多少，所以放心的使用吧，coding提供的是私有仓库还是可以放心，github进行进行相同操作就改一下配置也可以最后删掉ssh key。 coding+DaoCloud持续集成到此结束，有问题可以下方留言评论，或者很急的话发邮件提醒我也可以，邮箱在about me里。 另外还有 《Coding+Hexo搭建个人博客》 《Github+Appveyor博客云端持续集成》 《hexo博客部署到Github》 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>CI</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Coding+Hexo搭建个人博客]]></title>
    <url>%2Farchives%2F9997094b.html</url>
    <content type="text"><![CDATA[好了，答应了好几个评论的朋友要写一篇教程，不多说，我们直接进入主题。 我搭建博客用的是hexo框架，因为这个框架比较简单轻便，而且依赖于node.js管理包文件，我以前用过也用过一些npm的内容，所以选了这个。这里提一句，基于coding或者github搭建的博客都是静态页面，轻量简洁，相对的功能上不如Wordpress那样强大，但是我们也可以用第三方插件实现文章统计，网站计数，博客评论等功能，看自己喜好加吧。 好了，现在我们开始操作。 step 1 安装环境环境有三，node git hexo，hexo最后装。 git直接去下官网最新版本，安装步骤就不停next就好，不放心的话搜一下百度知道，有几个步骤需要斟酌，不过影响不大。链接在这里git官网。 node也是官网最新版本，node.js中文官网现在好像是8.x了，这个就一直next安装，装好后在桌面打开cmd， npm是node.js集成的包管理工具，现在是直接随node装好了。 查看版本号，成功显示就能用了，如果显示不是可用的命令就需要手动添加node环境变量，这个也简单，百度知道全有。 hexo在桌面右键，git bash，然后输入1npm install -g cnpm --registry=https://registry.npm.taobao.org 这里用cnpm是因为npm连接不太稳定，我用npm也是装了两次hexo才成功。而cnpm是淘宝团队提供的一个npm镜像库，国内访问非常快，以后的npm命令就在前面加一个c，使用方法完全相同。然后 1cnpm install hexo-cli -g 在某个盘下新建一个文件夹，取名随意，我是blog。 然后在这个文件夹下右键，git bash，然后 1hexo init 即在此初始化hexo源文件，需要这个blog文件夹初始为空。然后 1cnpm install 这一步是安装通用的npm包文件，如果有特定的npm包需要额外添加。我们除了通用包，还要一个hexo部署博客的包文件， 1cnpm install hexo-deployer-git --save 现在需要的基础包就安装好了。 step 2 测试本地发布现在新建一个博客，在blog文件夹下右键，git bash，然后输入 1hexo new test 这一步是在生成一篇空博客marksown文档，存在blog\source\_post路径下，可以用编辑器打开它，我用的是sublime，装了markdown editing和markdown preview插件，或者其他md编辑器都行，在test.md中随便写点什么，然后保存。接下来1hexo g 这一步是hexo的核心，把md文件转为静态页面，并添加主题样式和必要的链接，生成的文件在public下。然后 1hexo s 这样，就是在本地预览博客，在浏览器地址栏中输入 1http://localhost:4000 就可以看到结果了。现在只能自己浏览，想要让其他人也能看到，就需要部署到服务器上。租服务器不仅要花费，还要自己搭建web环境，太麻烦了，而且不适合学生党和技术不够的同学们，万幸github和coding都提供了静态页面解析的功能，所以我们把public文件夹下的内容push到一个git远程仓库就可以了。现在我们需要开始发布到coding的步骤。 step 3 本地博客部署到coding首先，去官网登陆你的coding账号，没有就注册一个，然后完善个人信息，升级到银牌会员（才能绑定个人域名）。然后新建一个repository，项目名称就填你的用户名，选择私有，然后创建项目。 现在有了远程仓库，就要把本地仓库和远程仓库关联起来，首先在blog目录下git bash，然后输入1git config -l 查看你的git配置信息，像我的是这样123456789101112131415161718192021222324252627$ git config -lcore.symlinks=falsecore.autocrlf=truecore.fscache=truecolor.diff=autocolor.status=autocolor.branch=autocolor.interactive=truehelp.format=htmlrebase.autosquash=truehttp.sslcainfo=C:/Program Files/Git/mingw64/ssl/certs/ca-bundle.crtdiff.astextplain.textconv=astextplainfilter.lfs.clean=git-lfs clean -- %ffilter.lfs.smudge=git-lfs smudge -- %ffilter.lfs.required=truefilter.lfs.process=git-lfs filter-processcredential.helper=manageruser.name=xxxuser.email=xxx@xxx.comcore.repositoryformatversion=0core.filemode=falsecore.bare=falsecore.logallrefupdates=truecore.symlinks=falsecore.ignorecase=truegui.wmstate=normalgui.geometry=841x483+343+178 189 218 这里你只用关注的是这两行12user.name=xxxuser.email=xxx@xxx.com 如果你没有这两行，那么你需要添加配置：1git config --global user.email &quot;your email&quot; 和1git config --global user.name &quot;your name&quot; 将双引号中内容替换为你自己的coding用户名和邮箱，可以在coding个人设置中查看自己的用户名和邮箱。 然后我们给本地添加一个SSH key，这样的话每次部署就不用输密码。在git bash中输入1ssh-keygen -t rsa -b 4096 -C &quot;your email&quot; 成功会出现以下代码：123# Creates a new ssh key, using the provided email as a label# Generating public/private rsa key pair.Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter] // 推荐使用默认地址,如果使用非默认地址可能需要配置 .ssh/config 然后一直回车，回车，回车，然后在 Coding.net 添加公钥本地打开 id_rsa.pub 文件（一般在c盘用户文件夹下，进入你的用户文件夹，有一个.ssh文件，打开其中的id_rsa.pub ），复制其中全部内容，添加到Coding账户“SSH 公钥”页面 中，公钥名称可以随意起名字。完成后点击“添加”，然后输入密码或动态码即可添加完成。这里要注意是账户的SSH公匙，而不是项目中的设置的部署公匙，切记。 现在验证一下是否添加SSH公匙成功，在git bash中输入1ssh -T git@git.coding.net 如果成功，会出现以下代码 123Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added ‘git.coding.net,61.146.73.68’ (RSA) to the list of kn own hosts.Enter passphrase for key ‘/c/Users/xxx/.ssh/id_rsa’: Coding.net Tips : [ Hello xxx! You have connected to Coding.net by SSH successfully! ] 现在就已经添加好了公匙，我们离博客部署到coding只差一步。 step 4 部署博客到coding首先打开blog文件夹下的_config.yml文件，这是我的配置，你需要修改的地方我都加了注释，别的不要动，还有就是要注意这里yml文件是用的yaml脚本语言，对语法要求很严格，每个:后面要加上空格，没空格会编译出错。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Yanss&apos;s Blog #改为你自己的网站名subtitle:description: Winner Winner, Chicken Dinner #改为你自己的描述语句，随便写author: Yanss #改成你自己的名字language: timezone: Asia/Shanghai# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://yanss.top #url改成&quot;xxx.coding.me&quot;，xxx是你的仓库名，也是你的用户名root: /permalink: :year/:month/:day/:title/permalink_defaults:# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render:# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: # Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Date / Time format## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DDtime_format: HH:mm:ss# Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next #这里是主题名，你的先不要变，后面换主题再改# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:yanss/yanss.github.io.git #repo这里写你的仓库地址，下面告诉怎么找 branch: master #就一个分支就不用改，默认master#这个feed是添加RSS订阅用的，这里你没有暂时不用写feed: type: atom path: atom.xml limit: 20 hub: content: #这个search是站内搜索的，也是没有就不用写search: path: search.xml field: post format: html limit: 10000 打开coding网站上你刚才创建的仓库，点击代码，左下角选择SSH方式访问仓库，复制那个链接，把它填到你的_config.yml的repo那里。 现在我们就可以开始部署博客了，记得部署之前最好清理一遍public文件夹,也就是这样 123hexo cleanhexo ghexo d 或者你也可以直接 1hexo d -g coding+hexo的博客部署操作就是这些了，有问题可以下方留言评论，或者很急的话发邮件提醒我也可以，邮箱在about me里。 另外还有 《Coding+DaoCloud持续集成》 《Github+Appveyor博客云端持续集成》 《hexo博客部署到Github》 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Coding</tag>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql基础语句]]></title>
    <url>%2Farchives%2F929f9824.html</url>
    <content type="text"><![CDATA[选择：1select * from table1 where 范围 插入：1insert into table1(field1,field2) values(value1,value2) 删除：1delete from table1 where 范围 更新：1update table1 set field1=value1 where 范围 查找：12select * from table1 where field1 like ’%value1%’ //like的语法很精妙，查资料!&apos; 排序：1select * from table1 order by field1,field2 [desc] 总数：1select count as totalcount from table1 求和：1select sum(field1) as sumvalue from table1 平均：1select avg(field1) as avgvalue from table1 最大：1select max(field1) as maxvalue from table1 最小：1select min(field1) as minvalue from table1 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>SQL</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS正则表达式]]></title>
    <url>%2Farchives%2Fbf9092f5.html</url>
    <content type="text"><![CDATA[正则表达式（英语：Regular Expression，在代码中常简写为regex、regexp或RE）使用单个字符串来描述、匹配一系列符合某个句法规则的字符串搜索模式。搜索模式可用于文本搜索和文本替换。 语法 1/正则表达式主体/修饰符(可选) 实例：1var patt = /abc/i /abc/i是一个正则表达式 abc 是一个正则表达式主体（用于检索） i是一个修饰符（搜索不区分大小写） 使用字符串方法search() 方法 用于检索字符串中指定的子字符串，或检索与正则表达式相匹配的子字符串，并返回子串的起始位置。 实例 使用正则表达式搜索”Runoob”字符串，且不区分大小写：123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;搜索字符串 &quot;runoob&quot;, 并显示匹配的起始位置：&lt;/p&gt;&lt;button onclick=&quot;myFunction()&quot;&gt;点我&lt;/button&gt;&lt;p id=&quot;demo&quot;&gt;&lt;/p&gt;&lt;script&gt;function myFunction() &#123; var str = &quot;Visit Runoob!&quot;; var n = str.search(/Runoob/i); document.getElementById(&quot;demo&quot;).innerHTML = n;&#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 输出结果为：16 replace() 方法 用于在字符串中用一些字符替换另一些字符，或替换一个与正则表达式匹配的子串。 实例 使用正则表达式且不区分大小写将字符串中的 Microsoft 替换为 Runoob : 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;&lt;title&gt;菜鸟教程(runoob.com)&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;p&gt;替换 &quot;microsoft&quot; 为 &quot;Runoob&quot; :&lt;/p&gt;&lt;button onclick=&quot;myFunction()&quot;&gt;点我&lt;/button&gt;&lt;p id=&quot;demo&quot;&gt;请访问 Microsoft!&lt;/p&gt;&lt;script&gt;function myFunction() &#123; var str = document.getElementById(&quot;demo&quot;).innerHTML; var txt = str.replace(/microsoft/i,&quot;Runoob&quot;); document.getElementById(&quot;demo&quot;).innerHTML = txt;&#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 结果输出为：1Visit Runoob! 使用RegExp对象 在 JavaScript 中，RegExp 对象是一个预定义了属性和方法的正则表达式对象。 test() 方法 用于检测一个字符串是否匹配某个模式。如果字符串中有匹配的值返回 true ，否则返回 false。 语法: 1RegExpObject.test(string) 实例： 1234567891011121314&lt;script&gt;var str=&quot;Hello world!&quot;;//look for &quot;Hello&quot;var patt=/Hello/g;var result=patt.test(str);document.write(&quot;Returned value: &quot; + result); //look for &quot;W3CSchool&quot;patt=/W3CSchool/g;result=patt.test(str);document.write(&quot;&lt;br&gt;Returned value: &quot; + result);&lt;/script&gt; 输出：12Returned value: trueReturned value: false exec() 方法 用于检索字符串中的正则表达式的匹配。如果字符串中有匹配的值返回该匹配值，否则返回 null。 语法：1RegExpObject.exec(string) 实例：1234567891011121314&lt;script&gt;var str=&quot;Hello world!&quot;;//look for &quot;Hello&quot;var patt=/Hello/g;var result=patt.exec(str);document.write(&quot;Returned value: &quot; + result); //look for &quot;W3Schools&quot;patt=/W3Schools/g;result=patt.exec(str);document.write(&quot;&lt;br&gt;Returned value: &quot; + result);&lt;/script&gt; 输出：12Returned value: HelloReturned value: null 正则表达式修饰符 修饰符 描述 i 执行对大小写不敏感的匹配 g 执行全局匹配 m 执行多行匹配 正则表达式模式方括号用于查找某个范围内的字符 表达式 描述 [abc] 查找方括号之间的任何字符 [0-9] 查找任何从0至9的数字 (x\ y) 查找任何以\ 分隔的选项 元字符是拥有特殊含义的字符 元字符 描述 \d 查找数字 \s 查找空白字符 \b 匹配单词边界 \uxxxx 查找以十六进制数xxxx规定的Unicode字符 量词 量词 描述 n+ 匹配任何包含至少一个n的字符串 n* 匹配任何包含零个或多个n的字符串 n? 匹配任何包含零个或一个n的字符串 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Front-End</category>
      </categories>
      <tags>
        <tag>JavaScrit</tag>
        <tag>regexp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSM框架各层关系]]></title>
    <url>%2Farchives%2F457836a3.html</url>
    <content type="text"><![CDATA[SSM包含持久层Dao和Model，业务层Service和ServiceImpl，表现层Controller和View，本文讲述这些不同层的作用和关系。 持久层：Dao层（mapper） DAO层主要是做数据持久层的工作，负责与数据库进行联络的一些任务都封装在此 DAO层的设计首先是设计DAO的接口 然后在Spring的配置文件中定义此接口的实现类 然后就可在模块中调用此接口来进行数据业务的处理，而不用关心此接口的具体实现类是哪个类，显得结构非常清晰 DAO层的数据源配置，以及有关数据库连接的参数都在Spring的配置文件中进行配置 业务层：Service层 Service层主要负责业务模块的逻辑应用设计 首先设计接口，再设计其实现的类 接着再在Spring的配置文件中配置其实现的关联。这样我们就可以在应用中调用Service接口来进行业务处理 Service层的业务实现，具体要调用到已定义的DAO层的接口 封装Service层的业务逻辑有利于通用的业务逻辑的独立性和重复利用性，程序显得非常简洁 表现层：Controller层（Handler） Controller层负责具体的业务模块流程的控制 在此层里面要调用Service层的接口来控制业务流程 控制的配置也同样是在Spring的配置文件里面进行，针对具体的业务流程，会有不同的控制器，我们具体的设计过程中可以将流程进行抽象归纳，设计出可以重复利用的子单元流程模块，这样不仅使程序结构变得清晰，也大大减少了代码量 View层 View层：此层与控制层结合比较紧密，需要二者结合起来协同工发。View层主要负责前台jsp页面的表示. 各层联系 DAO层，Service层这两个层次都可以单独开发，互相的耦合度很低，完全可以独立进行，这样的一种模式在开发大项目的过程中尤其有优势 Controller，View层因为耦合度比较高，因而要结合在一起开发，但是也可以看作一个整体独立于前两个层进行开发。这样，在层与层之前我们只需要知道接口的定义，调用接口即可完成所需要的逻辑单元应用，一切显得非常清晰简单 Service层是建立在DAO层之上的，建立了DAO层后才可以建立Service层，而Service层又是在Controller层之下的，因而Service层应该既调用DAO层的接口，又要提供接口给Controller层的类来进行调用，它刚好处于一个中间层的位置。每个模型都有一个Service接口，每个接口分别封装各自的业务处理方法 总结：view层：结合control层，显示前台页面 control层：业务模块流程控制，调用service层接口 service层：业务操作实现类，调用dao接口 dao层：数据业务处理，持久化操作 model层：pojo， or mapping，持久层 123456graph TDA[View]--&gt;B[Controller]B--&gt;C[Service]C--&gt;D[ServiceImpl]D--&gt;E[Dao]E--&gt;F[Mapping] 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>SpringMVC</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树和二叉树]]></title>
    <url>%2Farchives%2F7b216a3b.html</url>
    <content type="text"><![CDATA[树树的定义 （递归）一棵树是一些节点的集合。这个集合可以是空集；若不是空集，则树由称作根的节点 r 以及 0 个或多个非空的（子）树 $T_1，T_2，···，T_k$ 组成，这些子树中每一棵的根都被来自根 r 的一条有向边所连结。 树的实现1234567//树节点的声明class TreeNode&#123; Object element; TreeNode firstChild; TreeNode netSibling;&#125; 将每个节点的所有儿子都放到树节点的链表中。 树的遍历 先序遍历 后序遍历 中序遍历 二叉树 二叉树（binary tree）是一棵树，其中每个节点都不能有多于两个的儿子。 二叉树平均深度为 $O(\sqrt{N})$，最大深度为 $N$。二叉查找树的平均深度为 $O(log N)$。 12345678//二叉树节点类class BinaryNode&#123; //Friendly data;accessible by other package toutines Object element;//The data in the node BinaryNode left;//Left child BinaryNode right;//right child&#125; 查找树ADT——二叉查找树 使二叉树成为查找树的性质是，对于树中的每个节点 X ，它的左子树中所有项的值小于 X 中的项，而它的右子树中所有项的值大于 X 中的项。 1234567891011121314//BinaryNode类private static class BinaryNode&lt;AnyType&gt;&#123; //Constructors BinaryNode(AnyType theElement) &#123;this(theElement, null, null);&#125; BinaryNode(AnyType theElement, BinaryNode&lt;AnyType&gt; lt, BinaryNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt;&#125; AnyType element;//The data in the node BinaryNode&lt;AnyType&gt; left;//Left child BinaryNode&lt;AnyType&gt; right;//Right child&#125; 二叉查找树架构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127//二叉查找树架构public class BinarySearchTree&lt;AnyType extends comparable&lt;? super AnyType&gt;&gt;&#123; private static class BinaryNode&lt;AnyType&gt; &#123; //Constructors BinaryNode(AnyType theElement) &#123;this(theElement, null, null);&#125; BinaryNode(AnyType theElement, BinaryNode&lt;AnyType&gt; lt, BinaryNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt;&#125; AnyType element;//The data in the node BinaryNode&lt;AnyType&gt; left;//Left child BinaryNode&lt;AnyType&gt; right;//Right child &#125; private BinaryNode&lt;AnyType&gt; root; public BinarySearchTree() &#123; root = null; &#125; public void makeEmpty() &#123; root = null; &#125; public boolean isEmpty() &#123; return root == null; &#125; public boolean contains( AnyType x ) &#123; return contains( x, root ); &#125; public AnyType findMin() &#123; if (isEmpty()) throw new UnderflowException(); return findMin(root).element; &#125; public AnyType finMax() &#123; if (isEmpty()) throw new UnderflowException(); return finMax(roow).element; &#125; public void insert(AnyType x) &#123; root = insert(x,root); &#125; public void remove(AnyType x) &#123; root = remove(x,root); &#125; public void printTree() &#123; if (isEmpty()) System.out.println("Empty tree"); else printTree(root); &#125; private boolean contains(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if (t == null) return false; int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) return contains(x, t.left); else if(compareResult &gt; 0) return contains(x, t.right); else return true; //Match &#125; private BinaryNode&lt;AnyType&gt; findMin(BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return null; else if(t.left == null) return t; return findMin(t.left); &#125; private BinaryNode&lt;AnyType&gt; finMax(BinaryNode&lt;AnyType&gt; t) &#123; if(t != null) while(t.right != null) t = t.right; return t; &#125; private BinaryNode&lt;AnyType&gt; insert(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return new BinaryNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return t; &#125; private BinaryNode&lt;AnyType&gt; remove(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Two children &#123; t.element = findMin(t.right).element; t.right = remove(t.element, t.right); &#125; else t = (t.left != null) ? t.left : t.right; return t; &#125; private void printTree(BinaryNode&lt;AnyType&gt; t) &#123; if (t != null) &#123; printTree(t.left); System.out.println(t.element); printTree(t.right); &#125; &#125;&#125; contains方法 如果树 $T$ 中含有项 $X$ 的节点，那么这个操作需要返回true，如果这样的节点不存在则返回false。树的结构使这种操作很简单。如果 $T$ 是空集，那么久返回false。否则，如果存储在 $T$ 处的项是 $X$ ，那么可以返回true。否则，我们对数 $T$ 的左子树或右子树进行一次递归调用，则依赖于 $X$ 与存储在 $T$ 中的项的关系。 123456789101112131415161718192021/** * Internal method to find an item in a subtree * @param x is item to search for. * @param t the node that roots the subtree. * @return true if the item is found; false otherwise. *///二叉查找树的contains操作private boolean contains(AnyType x, BinaryNode&lt;AnyType&gt; t) &#123; if (t == null) return false; int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) return contains(x, t.left); else if(compareResult &gt; 0) return contains(x, t.right); else return true; //Match &#125; 123456//递归用while循环代替 while(compareResult &lt;0) &#123; t=t.left; compareResult = x.compareTo(t.element); &#125; 算法表达式的简明性是以速度的降低为代价的。 findMin方法和findMax方法 这两个方法分别返回树中包含最小元和最大元的节点的引用。为执行findMin，从根开始并且只要有左儿子就向左进行。 终止点就是最小的元素。findMax除分支朝向右儿子其余过程相同。 123456789101112131415161718192021222324252627//用递归编写findMin，用非递归编写findMax/*** Internal method to find the smallest item in a subtree* @param t the node that roots the subtree.* @return node containing the smallest item*/private BinaryNode&lt;AnyType&gt; findMin(BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return null; else if(t.left == null) return t; return findMin(t.left);&#125;/*** Internal method to find the largest item in a subtree* @param t the node that roots the subtree.* @return node containing the largest item.*/private BinaryNode&lt;AnyType&gt; finMax(BinaryNode&lt;AnyType&gt; t)&#123; if(t != null) while(t.right != null) t = t.right; return t; &#125; insert方法123456789101112131415161718192021/** * Internal method to insert into a subtree * @param x the item to insert * @param t the node that roots the subtree * @return the new root of the subtree */ private BinaryNode&lt;AnyType&gt; insert(AnyType x, BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return new BinaryNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return t;&#125; remove方法1234567891011121314151617181920212223242526/** * Internal method to remove from a subtree * @param x the item to remove. * @param t the node that roots the subtree. * @return the new root of the subtree */private BinaryNode&lt;AnyType&gt; remove(AnyType x, BinaryNode&lt;AnyType&gt; t)&#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Node that has two children &#123; t.element = findMin(t.right).element;//Find the minimum item of right subtree t.right = remove(t.element, t.right);//Remove the node of minimum item recursively &#125; else t = (t.left != null) ? t.left : t.right;//Node that has one children; parent of the node roots subtree of the node return t;&#125; 如果节点是树叶，可以直接删除。 如果节点有一个儿子，这该节点需要在其父节点调整自己的链以绕过该节点 如果节点有两个儿子，一般的删除策略是用其右子树的最小的数据代替该节点，并在右子树中递归地删除那个最小的节点 另外，如果删除的次数不多，通常使用的策略是懒惰删除（lazy deletion）：当一个元素要被删除时，它仍留在树中，而只是被标记为删除。 AVL树 AVL树是带有平衡条件的二叉查找树。这个平衡条件必须要容易保持，而且它保证树的深度须是 $O(log N)$ 。一个AVL树是其每个节点的左子树和右子树的高度最多差 1 的二叉查找树（空树的高度定义为 -1）。 可以知道，在高度为 $h$ 的AVL树中，最少节点数 $S(h)=S(h-1)+S(h-2)+1$ 给出。对于 $h=0, S(h)=1; h=1, S(h)=2$ 。函数 $S(h)$ 与斐波那契数密切相关。 那么重点来了，对于AVL树的插入操作，有可能破坏树的平衡性。这时候，我们就需要在这一步插入完成之前恢复平衡的性质。 可以知道，从插入的节点往上，逆行到根，若发生平衡信息改变，那么改变的节点一定在这条路径上。我们需要找出这个需要重新平衡的节点 $\alpha$ 。 对于节点 $\alpha$ ，不平衡条件可能出现在一下四种操作中： 对 $\alpha$ 的左儿子的左子树进行一次插入（LL）。 对 $\alpha$ 的左儿子的右子树进行一次插入（LR）。 对 $\alpha$ 的右儿子的左子树进行一次插入（RL）。 对 $\alpha$ 的右儿子的右子树进行一次插入（RR）。 对于1和4，是插入发生在外边的情况，通过对树的一次单旋转而完成调整。对于2和3，是插入发生在内部的情况，通过对树的一次双旋转而完成调整。 这里先对AvlNode类进行定义： 1234567891011121314private static class AvlNode&lt;AnyType&gt;&#123; //Constructors AvlNode(AnyType theElement) &#123;this(theElement, null, null);&#125; AvlNode(AnyType theElement, AvlNode&lt;AnyType&gt; lt, AvlNode&lt;AnyType&gt; rt) &#123;element = theElement; left = lt; right = rt; height = 0;&#125; AnyType element;//The data in the code AvlNode&lt;AnyType&gt; left;//Left child AvlNode&lt;AnyType&gt; right;//Right child int height;//Height&#125; 然后需要一个返回节点高度的方法： ​ 12345678//返回AVL树的节点高度/** * return the height of node t, or -1, if null. */private int height(AvlNode&lt;AnyType&gt; t)&#123; return t == null ? -1 : t.height;&#125; 单旋转 1234567891011121314151617/** * Rotate binary tree node with left child. * For AVL trees, this is a single rotation for case 1. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; RotationWithLeftChild(AvlNode&lt;AnyType&gt; k2) &#123; AVLTreeNode&lt;AnyType&gt; k1 = k2.left; k2.left = k1.right; k1.right = k2; k2.height = Math.max( height(k2.left), height(k2.right)) + 1; k1.height = Math.max( height(k1.left), k2.height) + 1; return k1; &#125; 1234567891011121314151617/** * Rotate binary tree node with right child. * For AVL trees, this is a single rotation for case 4. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; RotationWithRightChild(AvlNode&lt;AnyType&gt; k1) &#123; AVLTreeNode&lt;AnyType&gt; k2 = k1.right; k1.right = k2.left; k2.left = k1; k1.height = Math.max( height(k1.left), height(k1.right)) + 1; k1.height = Math.max( height(k2.right), k1.height) + 1; return k2; &#125; 双旋转 1234567891011/** * Double rotate binary tree node: first left child * with its right child; then node k3 with new left child. * For AVL trees, this is a double rotation for case 2. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; doubleWithLeftChild(AvlNode&lt;AnyType&gt; k3)&#123; k3.left = RotationWithRightChild(k3.left); return RotationWithLeftChild(k3);&#125; 1234567891011/** * Double rotate binary tree node: first right child * with its left child; then node k1 with new right child. * For AVL trees, this is a double rotation for case 3. * Update heights, then return new root. */private AvlNode&lt;AnyType&gt; doubleWithRightChild(AvlNode&lt;AnyType&gt; k1)&#123; k1.right = RotationWithRightChild(k1.right); return RotationWithLeftChild(k1);&#125; AVL树的插入方法 插入方法就是前文中的insert方法，只是在最后一行调用平衡的方法以保持AVL树的平衡性。123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Internal method to insert into a subtree. * @param x the item to insert. * @param t the node that roots the subtree. * @return the new root of the subtree. */private AvlNode&lt;AnyType&gt; insert(AnyType x, AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return new AvlNode&lt;&gt;(x, null, null); int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = insert(x, t.left); else if(compareResult &gt; 0) t.right = insert(x, t.right); else ;//Duplicate; do nothing return balance(t);&#125;private static final int ALLOWED_IMBALLANCE = 1;//Assume t is either balanced of within one of being balancedprivate AvlNode&lt;AnyType&gt; balance(AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return t; if(height(t.left) - height(t.right) &gt; ALLOWED_IMBALLANCE) if(height(t.left.left) &gt;= height(t.left.right)) t = RotationWithLeftChild(t); else t = doubleWithLeftChild(t); else if(height(t.right) - height(t.left) &gt; ALLOWED_IMBALLANCE) if(height(t.right.right) &gt;= height(t.right.left)) t = RotationWithRightChild(t); else t = doubleWithRightChild(t); t.height = Math.max(height(t.left), height(t.right)) + 1; return t;&#125; AVL树的删除方法 和AVL树的插入一样，只用在前文的删除方法最后加上一行调用平衡的方法即可。 1234567891011121314151617181920private AvlNode&lt;AnyType&gt; remove(AnyType x, AvlNode&lt;AnyType&gt; t)&#123; if(t == null) return t;//Item not found; do nothing int compareResult = x.compareTo(t.element); if(compareResult &lt; 0) t.left = remove(x, t.left); else if(compareResult &gt; 0) t.right = remove(x, t.right); else if(t.left != null &amp;&amp; t.right != null)//Node that has two children &#123; t.element = findMin(t.right).element;//Find the minimum item of right subtree t.right = remove(t.element, t.right);//Remove the node of minimum item recursively &#125; else t = (t.left != null) ? t.left : t.right;//Node that has one children; parent of the node roots subtree of the node return balance(t);&#125; 最后要说的是：博客源码 ， 欢迎 star]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>BinaryTree</tag>
      </tags>
  </entry>
</search>
